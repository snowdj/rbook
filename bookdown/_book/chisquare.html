<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 12 Categorical data analysis | Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)</title>
  <meta name="description" content="Learning Statistics with R covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students, focusing on the use of the R statistical software.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 12 Categorical data analysis | Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Learning Statistics with R covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students, focusing on the use of the R statistical software." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Categorical data analysis | Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)" />
  
  <meta name="twitter:description" content="Learning Statistics with R covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students, focusing on the use of the R statistical software." />
  

<meta name="author" content="Danielle Navarro (bookdown translation: Emily Kothe)">


<meta name="date" content="2019-01-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="part-v-statistical-tools.html">
<link rel="next" href="ttest.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<!-- ###### start inserted header ##### -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115940772-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-115940772-1');
</script>

<!-- add the twitter card and open graph tags -->
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@djnavarro">
<meta property="og:image" content="http://learningstatisticswithr.com/images/jasmine-faint.jpg">
<meta name="twitter:image" content="http://learningstatisticswithr.com/images/jasmine-faint.jpg">

<!-- ###### end inserted header ##### -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Learning Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="licensing.html"><a href="licensing.html"><i class="fa fa-check"></i>Licensing</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="preface.html"><a href="preface.html#preface-to-version-0.6.1"><i class="fa fa-check"></i><b>0.1</b> Preface to Version 0.6.1</a></li>
<li class="chapter" data-level="0.2" data-path="preface.html"><a href="preface.html#preface-to-version-0.6"><i class="fa fa-check"></i><b>0.2</b> Preface to Version 0.6</a></li>
<li class="chapter" data-level="0.3" data-path="preface.html"><a href="preface.html#preface-to-version-0.5"><i class="fa fa-check"></i><b>0.3</b> Preface to Version 0.5</a></li>
<li class="chapter" data-level="0.4" data-path="preface.html"><a href="preface.html#preface-to-version-0.4"><i class="fa fa-check"></i><b>0.4</b> Preface to Version 0.4</a></li>
<li class="chapter" data-level="0.5" data-path="preface.html"><a href="preface.html#preface-to-version-0.3"><i class="fa fa-check"></i><b>0.5</b> Preface to Version 0.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-i-background.html"><a href="part-i-background.html"><i class="fa fa-check"></i>Part I. Background</a></li>
<li class="chapter" data-level="1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#whywhywhy"><i class="fa fa-check"></i><b>1.1</b> On the psychology of statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1.1</b> The curse of belief bias</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="studydesign.html"><a href="studydesign.html"><i class="fa fa-check"></i><b>2</b> A brief introduction to research design</a><ul>
<li class="chapter" data-level="2.1" data-path="studydesign.html"><a href="studydesign.html#measurement"><i class="fa fa-check"></i><b>2.1</b> Introduction to psychological measurement</a><ul>
<li class="chapter" data-level="2.1.1" data-path="studydesign.html"><a href="studydesign.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>2.1.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="2.1.2" data-path="studydesign.html"><a href="studydesign.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>2.1.2</b> Operationalisation: defining your measurement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="studydesign.html"><a href="studydesign.html#scales"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a><ul>
<li class="chapter" data-level="2.2.1" data-path="studydesign.html"><a href="studydesign.html#nominal-scale"><i class="fa fa-check"></i><b>2.2.1</b> Nominal scale</a></li>
<li class="chapter" data-level="2.2.2" data-path="studydesign.html"><a href="studydesign.html#ordinal-scale"><i class="fa fa-check"></i><b>2.2.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="2.2.3" data-path="studydesign.html"><a href="studydesign.html#interval-scale"><i class="fa fa-check"></i><b>2.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="2.2.4" data-path="studydesign.html"><a href="studydesign.html#ratio-scale"><i class="fa fa-check"></i><b>2.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="2.2.5" data-path="studydesign.html"><a href="studydesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>2.2.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="studydesign.html"><a href="studydesign.html#some-complexities"><i class="fa fa-check"></i><b>2.2.6</b> Some complexities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="studydesign.html"><a href="studydesign.html#reliability"><i class="fa fa-check"></i><b>2.3</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="2.4" data-path="studydesign.html"><a href="studydesign.html#ivdv"><i class="fa fa-check"></i><b>2.4</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="2.5" data-path="studydesign.html"><a href="studydesign.html#researchdesigns"><i class="fa fa-check"></i><b>2.5</b> Experimental and non-experimental research</a><ul>
<li class="chapter" data-level="2.5.1" data-path="studydesign.html"><a href="studydesign.html#experimental-research"><i class="fa fa-check"></i><b>2.5.1</b> Experimental research</a></li>
<li class="chapter" data-level="2.5.2" data-path="studydesign.html"><a href="studydesign.html#non-experimental-research"><i class="fa fa-check"></i><b>2.5.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="studydesign.html"><a href="studydesign.html#validity"><i class="fa fa-check"></i><b>2.6</b> Assessing the validity of a study</a><ul>
<li class="chapter" data-level="2.6.1" data-path="studydesign.html"><a href="studydesign.html#internal-validity"><i class="fa fa-check"></i><b>2.6.1</b> Internal validity</a></li>
<li class="chapter" data-level="2.6.2" data-path="studydesign.html"><a href="studydesign.html#external-validity"><i class="fa fa-check"></i><b>2.6.2</b> External validity</a></li>
<li class="chapter" data-level="2.6.3" data-path="studydesign.html"><a href="studydesign.html#construct-validity"><i class="fa fa-check"></i><b>2.6.3</b> Construct validity</a></li>
<li class="chapter" data-level="2.6.4" data-path="studydesign.html"><a href="studydesign.html#face-validity"><i class="fa fa-check"></i><b>2.6.4</b> Face validity</a></li>
<li class="chapter" data-level="2.6.5" data-path="studydesign.html"><a href="studydesign.html#ecological-validity"><i class="fa fa-check"></i><b>2.6.5</b> Ecological validity</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="studydesign.html"><a href="studydesign.html#confounds-artifacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>2.7</b> Confounds, artifacts and other threats to validity</a><ul>
<li class="chapter" data-level="2.7.1" data-path="studydesign.html"><a href="studydesign.html#history-effects"><i class="fa fa-check"></i><b>2.7.1</b> History effects</a></li>
<li class="chapter" data-level="2.7.2" data-path="studydesign.html"><a href="studydesign.html#maturation-effects"><i class="fa fa-check"></i><b>2.7.2</b> Maturation effects</a></li>
<li class="chapter" data-level="2.7.3" data-path="studydesign.html"><a href="studydesign.html#repeated-testing-effects"><i class="fa fa-check"></i><b>2.7.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="studydesign.html"><a href="studydesign.html#selection-bias"><i class="fa fa-check"></i><b>2.7.4</b> Selection bias</a></li>
<li class="chapter" data-level="2.7.5" data-path="studydesign.html"><a href="studydesign.html#differentialattrition"><i class="fa fa-check"></i><b>2.7.5</b> Differential attrition</a></li>
<li class="chapter" data-level="2.7.6" data-path="studydesign.html"><a href="studydesign.html#non-response-bias"><i class="fa fa-check"></i><b>2.7.6</b> Non-response bias</a></li>
<li class="chapter" data-level="2.7.7" data-path="studydesign.html"><a href="studydesign.html#regression-to-the-mean"><i class="fa fa-check"></i><b>2.7.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="2.7.8" data-path="studydesign.html"><a href="studydesign.html#experimenter-bias"><i class="fa fa-check"></i><b>2.7.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="2.7.9" data-path="studydesign.html"><a href="studydesign.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>2.7.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="2.7.10" data-path="studydesign.html"><a href="studydesign.html#placebo-effects"><i class="fa fa-check"></i><b>2.7.10</b> Placebo effects</a></li>
<li class="chapter" data-level="2.7.11" data-path="studydesign.html"><a href="studydesign.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>2.7.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="2.7.12" data-path="studydesign.html"><a href="studydesign.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>2.7.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="studydesign.html"><a href="studydesign.html#summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-an-introduction-to-r.html"><a href="part-ii-an-introduction-to-r.html"><i class="fa fa-check"></i>Part II. An introduction to R</a></li>
<li class="chapter" data-level="3" data-path="introR.html"><a href="introR.html"><i class="fa fa-check"></i><b>3</b> Getting started with R</a><ul>
<li class="chapter" data-level="3.1" data-path="introR.html"><a href="introR.html#gettingR"><i class="fa fa-check"></i><b>3.1</b> Installing R</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introR.html"><a href="introR.html#installing-r-on-a-windows-computer"><i class="fa fa-check"></i><b>3.1.1</b> Installing R on a Windows computer</a></li>
<li class="chapter" data-level="3.1.2" data-path="introR.html"><a href="introR.html#installing-r-on-a-mac"><i class="fa fa-check"></i><b>3.1.2</b> Installing R on a Mac</a></li>
<li class="chapter" data-level="3.1.3" data-path="introR.html"><a href="introR.html#installing-r-on-a-linux-computer"><i class="fa fa-check"></i><b>3.1.3</b> Installing R on a Linux computer</a></li>
<li class="chapter" data-level="3.1.4" data-path="introR.html"><a href="introR.html#installingrstudio"><i class="fa fa-check"></i><b>3.1.4</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="3.1.5" data-path="introR.html"><a href="introR.html#startingR"><i class="fa fa-check"></i><b>3.1.5</b> Starting up R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introR.html"><a href="introR.html#firstcommand"><i class="fa fa-check"></i><b>3.2</b> Typing commands at the R console</a><ul>
<li class="chapter" data-level="3.2.1" data-path="introR.html"><a href="introR.html#an-important-digression-about-formatting"><i class="fa fa-check"></i><b>3.2.1</b> An important digression about formatting</a></li>
<li class="chapter" data-level="3.2.2" data-path="introR.html"><a href="introR.html#be-very-careful-to-avoid-typos"><i class="fa fa-check"></i><b>3.2.2</b> Be very careful to avoid typos</a></li>
<li class="chapter" data-level="3.2.3" data-path="introR.html"><a href="introR.html#r-is-a-bit-flexible-with-spacing"><i class="fa fa-check"></i><b>3.2.3</b> R is (a bit) flexible with spacing</a></li>
<li class="chapter" data-level="3.2.4" data-path="introR.html"><a href="introR.html#r-can-sometimes-tell-that-youre-not-finished-yet-but-not-often"><i class="fa fa-check"></i><b>3.2.4</b> R can sometimes tell that you’re not finished yet (but not often)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="introR.html"><a href="introR.html#arithmetic"><i class="fa fa-check"></i><b>3.3</b> Doing simple calculations with R</a><ul>
<li class="chapter" data-level="3.3.1" data-path="introR.html"><a href="introR.html#adding-subtracting-multiplying-and-dividing"><i class="fa fa-check"></i><b>3.3.1</b> Adding, subtracting, multiplying and dividing</a></li>
<li class="chapter" data-level="3.3.2" data-path="introR.html"><a href="introR.html#taking-powers"><i class="fa fa-check"></i><b>3.3.2</b> Taking powers</a></li>
<li class="chapter" data-level="3.3.3" data-path="introR.html"><a href="introR.html#bedmas"><i class="fa fa-check"></i><b>3.3.3</b> Doing calculations in the right order</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introR.html"><a href="introR.html#assign"><i class="fa fa-check"></i><b>3.4</b> Storing a number as a variable</a><ul>
<li class="chapter" data-level="3.4.1" data-path="introR.html"><a href="introR.html#variable-assignment-using---and--"><i class="fa fa-check"></i><b>3.4.1</b> Variable assignment using <code>&lt;-</code> and <code>-&gt;</code></a></li>
<li class="chapter" data-level="3.4.2" data-path="introR.html"><a href="introR.html#doing-calculations-using-variables"><i class="fa fa-check"></i><b>3.4.2</b> Doing calculations using variables</a></li>
<li class="chapter" data-level="3.4.3" data-path="introR.html"><a href="introR.html#rules-and-conventions-for-naming-variables"><i class="fa fa-check"></i><b>3.4.3</b> Rules and conventions for naming variables</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="introR.html"><a href="introR.html#usingfunctions"><i class="fa fa-check"></i><b>3.5</b> Using functions to do calculations</a><ul>
<li class="chapter" data-level="3.5.1" data-path="introR.html"><a href="introR.html#functionarguments"><i class="fa fa-check"></i><b>3.5.1</b> Function arguments, their names and their defaults</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="introR.html"><a href="introR.html#RStudio1"><i class="fa fa-check"></i><b>3.6</b> Letting RStudio help you with your commands</a><ul>
<li class="chapter" data-level="3.6.1" data-path="introR.html"><a href="introR.html#autocomplete-using-tab"><i class="fa fa-check"></i><b>3.6.1</b> Autocomplete using “tab”</a></li>
<li class="chapter" data-level="3.6.2" data-path="introR.html"><a href="introR.html#browsing-your-command-history"><i class="fa fa-check"></i><b>3.6.2</b> Browsing your command history</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="introR.html"><a href="introR.html#vectors"><i class="fa fa-check"></i><b>3.7</b> Storing many numbers as a vector</a><ul>
<li class="chapter" data-level="3.7.1" data-path="introR.html"><a href="introR.html#creating-a-vector"><i class="fa fa-check"></i><b>3.7.1</b> Creating a vector</a></li>
<li class="chapter" data-level="3.7.2" data-path="introR.html"><a href="introR.html#a-handy-digression"><i class="fa fa-check"></i><b>3.7.2</b> A handy digression</a></li>
<li class="chapter" data-level="3.7.3" data-path="introR.html"><a href="introR.html#vectorsubset"><i class="fa fa-check"></i><b>3.7.3</b> Getting information out of vectors</a></li>
<li class="chapter" data-level="3.7.4" data-path="introR.html"><a href="introR.html#altering-the-elements-of-a-vector"><i class="fa fa-check"></i><b>3.7.4</b> Altering the elements of a vector</a></li>
<li class="chapter" data-level="3.7.5" data-path="introR.html"><a href="introR.html#veclength"><i class="fa fa-check"></i><b>3.7.5</b> Useful things to know about vectors</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="introR.html"><a href="introR.html#text"><i class="fa fa-check"></i><b>3.8</b> Storing text data</a><ul>
<li class="chapter" data-level="3.8.1" data-path="introR.html"><a href="introR.html#simpletext"><i class="fa fa-check"></i><b>3.8.1</b> Working with text</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="introR.html"><a href="introR.html#logicals"><i class="fa fa-check"></i><b>3.9</b> Storing “true or false” data</a><ul>
<li class="chapter" data-level="3.9.1" data-path="introR.html"><a href="introR.html#assessing-mathematical-truths"><i class="fa fa-check"></i><b>3.9.1</b> Assessing mathematical truths</a></li>
<li class="chapter" data-level="3.9.2" data-path="introR.html"><a href="introR.html#logical-operations"><i class="fa fa-check"></i><b>3.9.2</b> Logical operations</a></li>
<li class="chapter" data-level="3.9.3" data-path="introR.html"><a href="introR.html#storing-and-using-logical-data"><i class="fa fa-check"></i><b>3.9.3</b> Storing and using logical data</a></li>
<li class="chapter" data-level="3.9.4" data-path="introR.html"><a href="introR.html#vectors-of-logicals"><i class="fa fa-check"></i><b>3.9.4</b> Vectors of logicals</a></li>
<li class="chapter" data-level="3.9.5" data-path="introR.html"><a href="introR.html#logictext"><i class="fa fa-check"></i><b>3.9.5</b> Applying logical operation to text</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="introR.html"><a href="introR.html#indexing"><i class="fa fa-check"></i><b>3.10</b> Indexing vectors</a><ul>
<li class="chapter" data-level="3.10.1" data-path="introR.html"><a href="introR.html#extracting-multiple-elements"><i class="fa fa-check"></i><b>3.10.1</b> Extracting multiple elements</a></li>
<li class="chapter" data-level="3.10.2" data-path="introR.html"><a href="introR.html#logical-indexing"><i class="fa fa-check"></i><b>3.10.2</b> Logical indexing</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="introR.html"><a href="introR.html#quitting-r"><i class="fa fa-check"></i><b>3.11</b> Quitting R</a></li>
<li class="chapter" data-level="3.12" data-path="introR.html"><a href="introR.html#summary-1"><i class="fa fa-check"></i><b>3.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>4</b> Additional R concepts</a><ul>
<li class="chapter" data-level="4.1" data-path="mechanics.html"><a href="mechanics.html#comments"><i class="fa fa-check"></i><b>4.1</b> Using comments</a></li>
<li class="chapter" data-level="4.2" data-path="mechanics.html"><a href="mechanics.html#packageinstall"><i class="fa fa-check"></i><b>4.2</b> Installing and loading packages</a><ul>
<li class="chapter" data-level="4.2.1" data-path="mechanics.html"><a href="mechanics.html#the-package-panel-in-rstudio"><i class="fa fa-check"></i><b>4.2.1</b> The package panel in RStudio</a></li>
<li class="chapter" data-level="4.2.2" data-path="mechanics.html"><a href="mechanics.html#packageload"><i class="fa fa-check"></i><b>4.2.2</b> Loading a package</a></li>
<li class="chapter" data-level="4.2.3" data-path="mechanics.html"><a href="mechanics.html#packageunload"><i class="fa fa-check"></i><b>4.2.3</b> Unloading a package</a></li>
<li class="chapter" data-level="4.2.4" data-path="mechanics.html"><a href="mechanics.html#a-few-extra-comments"><i class="fa fa-check"></i><b>4.2.4</b> A few extra comments</a></li>
<li class="chapter" data-level="4.2.5" data-path="mechanics.html"><a href="mechanics.html#downloading-new-packages"><i class="fa fa-check"></i><b>4.2.5</b> Downloading new packages</a></li>
<li class="chapter" data-level="4.2.6" data-path="mechanics.html"><a href="mechanics.html#updating-r-and-r-packages"><i class="fa fa-check"></i><b>4.2.6</b> Updating R and R packages</a></li>
<li class="chapter" data-level="4.2.7" data-path="mechanics.html"><a href="mechanics.html#what-packages-does-this-book-use"><i class="fa fa-check"></i><b>4.2.7</b> What packages does this book use?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="mechanics.html"><a href="mechanics.html#workspace"><i class="fa fa-check"></i><b>4.3</b> Managing the workspace</a><ul>
<li class="chapter" data-level="4.3.1" data-path="mechanics.html"><a href="mechanics.html#listing-the-contents-of-the-workspace"><i class="fa fa-check"></i><b>4.3.1</b> Listing the contents of the workspace</a></li>
<li class="chapter" data-level="4.3.2" data-path="mechanics.html"><a href="mechanics.html#removing-variables-from-the-workspace"><i class="fa fa-check"></i><b>4.3.2</b> Removing variables from the workspace</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="mechanics.html"><a href="mechanics.html#navigation"><i class="fa fa-check"></i><b>4.4</b> Navigating the file system</a><ul>
<li class="chapter" data-level="4.4.1" data-path="mechanics.html"><a href="mechanics.html#filesystem"><i class="fa fa-check"></i><b>4.4.1</b> The file system itself</a></li>
<li class="chapter" data-level="4.4.2" data-path="mechanics.html"><a href="mechanics.html#navigationR"><i class="fa fa-check"></i><b>4.4.2</b> Navigating the file system using the R console</a></li>
<li class="chapter" data-level="4.4.3" data-path="mechanics.html"><a href="mechanics.html#why-do-the-windows-paths-use-the-wrong-slash"><i class="fa fa-check"></i><b>4.4.3</b> Why do the Windows paths use the wrong slash?</a></li>
<li class="chapter" data-level="4.4.4" data-path="mechanics.html"><a href="mechanics.html#nav3"><i class="fa fa-check"></i><b>4.4.4</b> Navigating the file system using the RStudio file panel</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="mechanics.html"><a href="mechanics.html#load"><i class="fa fa-check"></i><b>4.5</b> Loading and saving data</a><ul>
<li class="chapter" data-level="4.5.1" data-path="mechanics.html"><a href="mechanics.html#loading-workspace-files-using-r"><i class="fa fa-check"></i><b>4.5.1</b> Loading workspace files using R</a></li>
<li class="chapter" data-level="4.5.2" data-path="mechanics.html"><a href="mechanics.html#loading-workspace-files-using-rstudio"><i class="fa fa-check"></i><b>4.5.2</b> Loading workspace files using RStudio</a></li>
<li class="chapter" data-level="4.5.3" data-path="mechanics.html"><a href="mechanics.html#loadingcsv"><i class="fa fa-check"></i><b>4.5.3</b> Importing data from CSV files using loadingcsv</a></li>
<li class="chapter" data-level="4.5.4" data-path="mechanics.html"><a href="mechanics.html#importing-data-from-csv-files-using-rstudio"><i class="fa fa-check"></i><b>4.5.4</b> Importing data from CSV files using RStudio</a></li>
<li class="chapter" data-level="4.5.5" data-path="mechanics.html"><a href="mechanics.html#saving-a-workspace-file-using-save"><i class="fa fa-check"></i><b>4.5.5</b> Saving a workspace file using <code>save</code></a></li>
<li class="chapter" data-level="4.5.6" data-path="mechanics.html"><a href="mechanics.html#save1"><i class="fa fa-check"></i><b>4.5.6</b> Saving a workspace file using RStudio</a></li>
<li class="chapter" data-level="4.5.7" data-path="mechanics.html"><a href="mechanics.html#other-things-you-might-want-to-save"><i class="fa fa-check"></i><b>4.5.7</b> Other things you might want to save</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="mechanics.html"><a href="mechanics.html#useful"><i class="fa fa-check"></i><b>4.6</b> Useful things to know about variables</a><ul>
<li class="chapter" data-level="4.6.1" data-path="mechanics.html"><a href="mechanics.html#specials"><i class="fa fa-check"></i><b>4.6.1</b> Special values</a></li>
<li class="chapter" data-level="4.6.2" data-path="mechanics.html"><a href="mechanics.html#names"><i class="fa fa-check"></i><b>4.6.2</b> Assigning names to vector elements</a></li>
<li class="chapter" data-level="4.6.3" data-path="mechanics.html"><a href="mechanics.html#variable-classes"><i class="fa fa-check"></i><b>4.6.3</b> Variable classes</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="mechanics.html"><a href="mechanics.html#factors"><i class="fa fa-check"></i><b>4.7</b> Factors</a><ul>
<li class="chapter" data-level="4.7.1" data-path="mechanics.html"><a href="mechanics.html#introducing-factors"><i class="fa fa-check"></i><b>4.7.1</b> Introducing factors</a></li>
<li class="chapter" data-level="4.7.2" data-path="mechanics.html"><a href="mechanics.html#labelling-the-factor-levels"><i class="fa fa-check"></i><b>4.7.2</b> Labelling the factor levels</a></li>
<li class="chapter" data-level="4.7.3" data-path="mechanics.html"><a href="mechanics.html#moving-on"><i class="fa fa-check"></i><b>4.7.3</b> Moving on…</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="mechanics.html"><a href="mechanics.html#dataframes"><i class="fa fa-check"></i><b>4.8</b> Data frames</a><ul>
<li class="chapter" data-level="4.8.1" data-path="mechanics.html"><a href="mechanics.html#introducing-data-frames"><i class="fa fa-check"></i><b>4.8.1</b> Introducing data frames</a></li>
<li class="chapter" data-level="4.8.2" data-path="mechanics.html"><a href="mechanics.html#pulling-out-the-contents-of-the-data-frame-using"><i class="fa fa-check"></i><b>4.8.2</b> Pulling out the contents of the data frame using <code>$</code></a></li>
<li class="chapter" data-level="4.8.3" data-path="mechanics.html"><a href="mechanics.html#getting-information-about-a-data-frame"><i class="fa fa-check"></i><b>4.8.3</b> Getting information about a data frame</a></li>
<li class="chapter" data-level="4.8.4" data-path="mechanics.html"><a href="mechanics.html#looking-for-more-on-data-frames"><i class="fa fa-check"></i><b>4.8.4</b> Looking for more on data frames?</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="mechanics.html"><a href="mechanics.html#lists"><i class="fa fa-check"></i><b>4.9</b> Lists</a></li>
<li class="chapter" data-level="4.10" data-path="mechanics.html"><a href="mechanics.html#formulas"><i class="fa fa-check"></i><b>4.10</b> Formulas</a></li>
<li class="chapter" data-level="4.11" data-path="mechanics.html"><a href="mechanics.html#generics"><i class="fa fa-check"></i><b>4.11</b> Generic functions</a></li>
<li class="chapter" data-level="4.12" data-path="mechanics.html"><a href="mechanics.html#help"><i class="fa fa-check"></i><b>4.12</b> Getting help</a><ul>
<li class="chapter" data-level="4.12.1" data-path="mechanics.html"><a href="mechanics.html#how-to-read-the-help-documentation"><i class="fa fa-check"></i><b>4.12.1</b> How to read the help documentation</a></li>
<li class="chapter" data-level="4.12.2" data-path="mechanics.html"><a href="mechanics.html#other-resources"><i class="fa fa-check"></i><b>4.12.2</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="mechanics.html"><a href="mechanics.html#summary-2"><i class="fa fa-check"></i><b>4.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-working-with-data.html"><a href="part-iii-working-with-data.html"><i class="fa fa-check"></i>Part III. Working with data</a></li>
<li class="chapter" data-level="5" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>5</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="descriptives.html"><a href="descriptives.html#centraltendency"><i class="fa fa-check"></i><b>5.1</b> Measures of central tendency</a><ul>
<li class="chapter" data-level="5.1.1" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>5.1.1</b> The mean</a></li>
<li class="chapter" data-level="5.1.2" data-path="descriptives.html"><a href="descriptives.html#calculating-the-mean-in-r"><i class="fa fa-check"></i><b>5.1.2</b> Calculating the mean in R</a></li>
<li class="chapter" data-level="5.1.3" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>5.1.3</b> The median</a></li>
<li class="chapter" data-level="5.1.4" data-path="descriptives.html"><a href="descriptives.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>5.1.4</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="5.1.5" data-path="descriptives.html"><a href="descriptives.html#housingpriceexample"><i class="fa fa-check"></i><b>5.1.5</b> A real life example</a></li>
<li class="chapter" data-level="5.1.6" data-path="descriptives.html"><a href="descriptives.html#trimmedmean"><i class="fa fa-check"></i><b>5.1.6</b> Trimmed mean</a></li>
<li class="chapter" data-level="5.1.7" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>5.1.7</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="descriptives.html"><a href="descriptives.html#var"><i class="fa fa-check"></i><b>5.2</b> Measures of variability</a><ul>
<li class="chapter" data-level="5.2.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>5.2.1</b> Range</a></li>
<li class="chapter" data-level="5.2.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>5.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="5.2.3" data-path="descriptives.html"><a href="descriptives.html#aad"><i class="fa fa-check"></i><b>5.2.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="5.2.4" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>5.2.4</b> Variance</a></li>
<li class="chapter" data-level="5.2.5" data-path="descriptives.html"><a href="descriptives.html#sd"><i class="fa fa-check"></i><b>5.2.5</b> Standard deviation</a></li>
<li class="chapter" data-level="5.2.6" data-path="descriptives.html"><a href="descriptives.html#mad"><i class="fa fa-check"></i><b>5.2.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="5.2.7" data-path="descriptives.html"><a href="descriptives.html#which-measure-to-use"><i class="fa fa-check"></i><b>5.2.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="descriptives.html"><a href="descriptives.html#skewandkurtosis"><i class="fa fa-check"></i><b>5.3</b> Skew and kurtosis</a></li>
<li class="chapter" data-level="5.4" data-path="studydesign.html"><a href="studydesign.html#summary"><i class="fa fa-check"></i><b>5.4</b> Getting an overall summary of a variable</a><ul>
<li class="chapter" data-level="5.4.1" data-path="descriptives.html"><a href="descriptives.html#summarising-a-variable"><i class="fa fa-check"></i><b>5.4.1</b> “Summarising” a variable</a></li>
<li class="chapter" data-level="5.4.2" data-path="descriptives.html"><a href="descriptives.html#summarising-a-data-frame"><i class="fa fa-check"></i><b>5.4.2</b> “Summarising” a data frame</a></li>
<li class="chapter" data-level="5.4.3" data-path="descriptives.html"><a href="descriptives.html#describing-a-data-frame"><i class="fa fa-check"></i><b>5.4.3</b> “Describing” a data frame</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="descriptives.html"><a href="descriptives.html#groupdescriptives"><i class="fa fa-check"></i><b>5.5</b> Descriptive statistics separately for each group</a></li>
<li class="chapter" data-level="5.6" data-path="descriptives.html"><a href="descriptives.html#zscore"><i class="fa fa-check"></i><b>5.6</b> Standard scores</a></li>
<li class="chapter" data-level="5.7" data-path="descriptives.html"><a href="descriptives.html#correl"><i class="fa fa-check"></i><b>5.7</b> Correlations</a><ul>
<li class="chapter" data-level="5.7.1" data-path="descriptives.html"><a href="descriptives.html#the-data"><i class="fa fa-check"></i><b>5.7.1</b> The data</a></li>
<li class="chapter" data-level="5.7.2" data-path="descriptives.html"><a href="descriptives.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>5.7.2</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="5.7.3" data-path="descriptives.html"><a href="descriptives.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>5.7.3</b> The correlation coefficient</a></li>
<li class="chapter" data-level="5.7.4" data-path="descriptives.html"><a href="descriptives.html#calculating-correlations-in-r"><i class="fa fa-check"></i><b>5.7.4</b> Calculating correlations in R</a></li>
<li class="chapter" data-level="5.7.5" data-path="descriptives.html"><a href="descriptives.html#interpretingcorrelations"><i class="fa fa-check"></i><b>5.7.5</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="5.7.6" data-path="descriptives.html"><a href="descriptives.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>5.7.6</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="5.7.7" data-path="descriptives.html"><a href="descriptives.html#the-correlate-function"><i class="fa fa-check"></i><b>5.7.7</b> The <code>correlate()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="descriptives.html"><a href="descriptives.html#missing"><i class="fa fa-check"></i><b>5.8</b> Handling missing values</a><ul>
<li class="chapter" data-level="5.8.1" data-path="descriptives.html"><a href="descriptives.html#the-single-variable-case"><i class="fa fa-check"></i><b>5.8.1</b> The single variable case</a></li>
<li class="chapter" data-level="5.8.2" data-path="descriptives.html"><a href="descriptives.html#missing-values-in-pairwise-calculations"><i class="fa fa-check"></i><b>5.8.2</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="descriptives.html"><a href="descriptives.html#summary-3"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
<li class="chapter" data-level="5.10" data-path="descriptives.html"><a href="descriptives.html#epilogue-good-descriptive-statistics-are-descriptive"><i class="fa fa-check"></i><b>5.10</b> Epilogue: Good descriptive statistics are descriptive!</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="graphics.html"><a href="graphics.html"><i class="fa fa-check"></i><b>6</b> Drawing graphs</a><ul>
<li class="chapter" data-level="6.1" data-path="graphics.html"><a href="graphics.html#rgraphics"><i class="fa fa-check"></i><b>6.1</b> An overview of R graphics</a></li>
<li class="chapter" data-level="6.2" data-path="graphics.html"><a href="graphics.html#introplotting"><i class="fa fa-check"></i><b>6.2</b> An introduction to plotting</a><ul>
<li class="chapter" data-level="6.2.1" data-path="graphics.html"><a href="graphics.html#a-tedious-digression"><i class="fa fa-check"></i><b>6.2.1</b> A tedious digression</a></li>
<li class="chapter" data-level="6.2.2" data-path="graphics.html"><a href="graphics.html#figtitles"><i class="fa fa-check"></i><b>6.2.2</b> Customising the title and the axis labels</a></li>
<li class="chapter" data-level="6.2.3" data-path="graphics.html"><a href="graphics.html#changing-the-plot-type"><i class="fa fa-check"></i><b>6.2.3</b> Changing the plot type</a></li>
<li class="chapter" data-level="6.2.4" data-path="graphics.html"><a href="graphics.html#changing-other-features-of-the-plot"><i class="fa fa-check"></i><b>6.2.4</b> Changing other features of the plot</a></li>
<li class="chapter" data-level="6.2.5" data-path="graphics.html"><a href="graphics.html#changing-the-appearance-of-the-axes"><i class="fa fa-check"></i><b>6.2.5</b> Changing the appearance of the axes</a></li>
<li class="chapter" data-level="6.2.6" data-path="graphics.html"><a href="graphics.html#dont-panic"><i class="fa fa-check"></i><b>6.2.6</b> Don’t panic</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="graphics.html"><a href="graphics.html#hist"><i class="fa fa-check"></i><b>6.3</b> Histograms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="graphics.html"><a href="graphics.html#visual-style-of-your-histogram"><i class="fa fa-check"></i><b>6.3.1</b> Visual style of your histogram</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="graphics.html"><a href="graphics.html#stem"><i class="fa fa-check"></i><b>6.4</b> Stem and leaf plots</a></li>
<li class="chapter" data-level="6.5" data-path="graphics.html"><a href="graphics.html#boxplots"><i class="fa fa-check"></i><b>6.5</b> Boxplots</a><ul>
<li class="chapter" data-level="6.5.1" data-path="graphics.html"><a href="graphics.html#visual-style-of-your-boxplot"><i class="fa fa-check"></i><b>6.5.1</b> Visual style of your boxplot</a></li>
<li class="chapter" data-level="6.5.2" data-path="graphics.html"><a href="graphics.html#boxplotoutliers"><i class="fa fa-check"></i><b>6.5.2</b> Using box plots to detect outliers</a></li>
<li class="chapter" data-level="6.5.3" data-path="graphics.html"><a href="graphics.html#multipleboxplots"><i class="fa fa-check"></i><b>6.5.3</b> Drawing multiple boxplots</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="graphics.html"><a href="graphics.html#scatterplots"><i class="fa fa-check"></i><b>6.6</b> Scatterplots</a><ul>
<li class="chapter" data-level="6.6.1" data-path="graphics.html"><a href="graphics.html#more-elaborate-options"><i class="fa fa-check"></i><b>6.6.1</b> More elaborate options</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="graphics.html"><a href="graphics.html#bargraph"><i class="fa fa-check"></i><b>6.7</b> Bar graphs</a><ul>
<li class="chapter" data-level="6.7.1" data-path="graphics.html"><a href="graphics.html#par"><i class="fa fa-check"></i><b>6.7.1</b> Changing global settings using par()</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="graphics.html"><a href="graphics.html#saveimage"><i class="fa fa-check"></i><b>6.8</b> Saving image files using R and Rstudio</a><ul>
<li class="chapter" data-level="6.8.1" data-path="graphics.html"><a href="graphics.html#the-ugly-details-advanced"><i class="fa fa-check"></i><b>6.8.1</b> The ugly details (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="graphics.html"><a href="graphics.html#summary-4"><i class="fa fa-check"></i><b>6.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="datahandling.html"><a href="datahandling.html"><i class="fa fa-check"></i><b>7</b> Pragmatic matters</a><ul>
<li class="chapter" data-level="7.1" data-path="datahandling.html"><a href="datahandling.html#freqtables"><i class="fa fa-check"></i><b>7.1</b> Tabulating and cross-tabulating data</a><ul>
<li class="chapter" data-level="7.1.1" data-path="datahandling.html"><a href="datahandling.html#creating-tables-from-vectors"><i class="fa fa-check"></i><b>7.1.1</b> Creating tables from vectors</a></li>
<li class="chapter" data-level="7.1.2" data-path="datahandling.html"><a href="datahandling.html#creating-tables-from-data-frames"><i class="fa fa-check"></i><b>7.1.2</b> Creating tables from data frames</a></li>
<li class="chapter" data-level="7.1.3" data-path="datahandling.html"><a href="datahandling.html#converting-a-table-of-counts-to-a-table-of-proportions"><i class="fa fa-check"></i><b>7.1.3</b> Converting a table of counts to a table of proportions</a></li>
<li class="chapter" data-level="7.1.4" data-path="datahandling.html"><a href="datahandling.html#low-level-tabulation"><i class="fa fa-check"></i><b>7.1.4</b> Low level tabulation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="datahandling.html"><a href="datahandling.html#transform"><i class="fa fa-check"></i><b>7.2</b> Transforming and recoding a variable</a><ul>
<li class="chapter" data-level="7.2.1" data-path="datahandling.html"><a href="datahandling.html#creating-a-transformed-variable"><i class="fa fa-check"></i><b>7.2.1</b> Creating a transformed variable</a></li>
<li class="chapter" data-level="7.2.2" data-path="datahandling.html"><a href="datahandling.html#cutting-a-numeric-variable-into-categories"><i class="fa fa-check"></i><b>7.2.2</b> Cutting a numeric variable into categories</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="datahandling.html"><a href="datahandling.html#mathfunc"><i class="fa fa-check"></i><b>7.3</b> A few more mathematical functions and operations</a><ul>
<li class="chapter" data-level="7.3.1" data-path="datahandling.html"><a href="datahandling.html#rounding-a-number"><i class="fa fa-check"></i><b>7.3.1</b> Rounding a number</a></li>
<li class="chapter" data-level="7.3.2" data-path="datahandling.html"><a href="datahandling.html#modulus-and-integer-division"><i class="fa fa-check"></i><b>7.3.2</b> Modulus and integer division</a></li>
<li class="chapter" data-level="7.3.3" data-path="datahandling.html"><a href="datahandling.html#logarithms-and-exponentials"><i class="fa fa-check"></i><b>7.3.3</b> Logarithms and exponentials</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="datahandling.html"><a href="datahandling.html#subset"><i class="fa fa-check"></i><b>7.4</b> Extracting a subset of a vector</a><ul>
<li class="chapter" data-level="7.4.1" data-path="datahandling.html"><a href="datahandling.html#refresher"><i class="fa fa-check"></i><b>7.4.1</b> Refresher</a></li>
<li class="chapter" data-level="7.4.2" data-path="datahandling.html"><a href="datahandling.html#using-in-to-match-multiple-cases"><i class="fa fa-check"></i><b>7.4.2</b> Using <code>%in%</code> to match multiple cases</a></li>
<li class="chapter" data-level="7.4.3" data-path="datahandling.html"><a href="datahandling.html#using-negative-indices-to-drop-elements"><i class="fa fa-check"></i><b>7.4.3</b> Using negative indices to drop elements</a></li>
<li class="chapter" data-level="7.4.4" data-path="datahandling.html"><a href="datahandling.html#splitting-a-vector-by-group"><i class="fa fa-check"></i><b>7.4.4</b> Splitting a vector by group</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="datahandling.html"><a href="datahandling.html#subsetdataframe"><i class="fa fa-check"></i><b>7.5</b> Extracting a subset of a data frame</a><ul>
<li class="chapter" data-level="7.5.1" data-path="datahandling.html"><a href="datahandling.html#using-the-subset-function"><i class="fa fa-check"></i><b>7.5.1</b> Using the <code>subset()</code> function</a></li>
<li class="chapter" data-level="7.5.2" data-path="datahandling.html"><a href="datahandling.html#using-square-brackets-i.-rows-and-columns"><i class="fa fa-check"></i><b>7.5.2</b> Using square brackets: I. Rows and columns</a></li>
<li class="chapter" data-level="7.5.3" data-path="datahandling.html"><a href="datahandling.html#using-square-brackets-ii.-some-elaborations"><i class="fa fa-check"></i><b>7.5.3</b> Using square brackets: II. Some elaborations</a></li>
<li class="chapter" data-level="7.5.4" data-path="datahandling.html"><a href="datahandling.html#dropping"><i class="fa fa-check"></i><b>7.5.4</b> Using square brackets: III. Understanding “dropping”</a></li>
<li class="chapter" data-level="7.5.5" data-path="datahandling.html"><a href="datahandling.html#using-square-brackets-iv.-columns-only"><i class="fa fa-check"></i><b>7.5.5</b> Using square brackets: IV. Columns only</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="datahandling.html"><a href="datahandling.html#sort"><i class="fa fa-check"></i><b>7.6</b> Sorting, flipping and merging data</a><ul>
<li class="chapter" data-level="7.6.1" data-path="datahandling.html"><a href="datahandling.html#sorting-a-numeric-or-character-vector"><i class="fa fa-check"></i><b>7.6.1</b> Sorting a numeric or character vector</a></li>
<li class="chapter" data-level="7.6.2" data-path="datahandling.html"><a href="datahandling.html#sorting-a-factor"><i class="fa fa-check"></i><b>7.6.2</b> Sorting a factor</a></li>
<li class="chapter" data-level="7.6.3" data-path="datahandling.html"><a href="datahandling.html#sortframe"><i class="fa fa-check"></i><b>7.6.3</b> Sorting a data frame</a></li>
<li class="chapter" data-level="7.6.4" data-path="datahandling.html"><a href="datahandling.html#binding-vectors-together"><i class="fa fa-check"></i><b>7.6.4</b> Binding vectors together</a></li>
<li class="chapter" data-level="7.6.5" data-path="datahandling.html"><a href="datahandling.html#binding-multiple-copies-of-the-same-vector-together"><i class="fa fa-check"></i><b>7.6.5</b> Binding multiple copies of the same vector together</a></li>
<li class="chapter" data-level="7.6.6" data-path="datahandling.html"><a href="datahandling.html#transposing-a-matrix-or-data-frame"><i class="fa fa-check"></i><b>7.6.6</b> Transposing a matrix or data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="datahandling.html"><a href="datahandling.html#reshape"><i class="fa fa-check"></i><b>7.7</b> Reshaping a data frame</a><ul>
<li class="chapter" data-level="7.7.1" data-path="datahandling.html"><a href="datahandling.html#long-form-and-wide-form-data"><i class="fa fa-check"></i><b>7.7.1</b> Long form and wide form data</a></li>
<li class="chapter" data-level="7.7.2" data-path="datahandling.html"><a href="datahandling.html#reshaping-data-using-widetolong"><i class="fa fa-check"></i><b>7.7.2</b> Reshaping data using <code>wideToLong()</code></a></li>
<li class="chapter" data-level="7.7.3" data-path="datahandling.html"><a href="datahandling.html#reshaping-data-using-longtowide"><i class="fa fa-check"></i><b>7.7.3</b> Reshaping data using <code>longToWide()</code></a></li>
<li class="chapter" data-level="7.7.4" data-path="datahandling.html"><a href="datahandling.html#reshaping-with-multiple-within-subject-factors"><i class="fa fa-check"></i><b>7.7.4</b> Reshaping with multiple within-subject factors</a></li>
<li class="chapter" data-level="7.7.5" data-path="datahandling.html"><a href="datahandling.html#what-other-options-are-there"><i class="fa fa-check"></i><b>7.7.5</b> What other options are there?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="datahandling.html"><a href="datahandling.html#textprocessing"><i class="fa fa-check"></i><b>7.8</b> Working with text</a><ul>
<li class="chapter" data-level="7.8.1" data-path="datahandling.html"><a href="datahandling.html#shortening-a-string"><i class="fa fa-check"></i><b>7.8.1</b> Shortening a string</a></li>
<li class="chapter" data-level="7.8.2" data-path="datahandling.html"><a href="datahandling.html#pasting-strings-together"><i class="fa fa-check"></i><b>7.8.2</b> Pasting strings together</a></li>
<li class="chapter" data-level="7.8.3" data-path="datahandling.html"><a href="datahandling.html#splitting-strings"><i class="fa fa-check"></i><b>7.8.3</b> Splitting strings</a></li>
<li class="chapter" data-level="7.8.4" data-path="datahandling.html"><a href="datahandling.html#making-simple-conversions"><i class="fa fa-check"></i><b>7.8.4</b> Making simple conversions</a></li>
<li class="chapter" data-level="7.8.5" data-path="datahandling.html"><a href="datahandling.html#logictext2"><i class="fa fa-check"></i><b>7.8.5</b> Applying logical operations to text</a></li>
<li class="chapter" data-level="7.8.6" data-path="datahandling.html"><a href="datahandling.html#concatenating-and-printing-with-cat"><i class="fa fa-check"></i><b>7.8.6</b> Concatenating and printing with <code>cat()</code></a></li>
<li class="chapter" data-level="7.8.7" data-path="datahandling.html"><a href="datahandling.html#escapechars"><i class="fa fa-check"></i><b>7.8.7</b> Using escape characters in text</a></li>
<li class="chapter" data-level="7.8.8" data-path="datahandling.html"><a href="datahandling.html#matching-and-substituting-text"><i class="fa fa-check"></i><b>7.8.8</b> Matching and substituting text</a></li>
<li class="chapter" data-level="7.8.9" data-path="datahandling.html"><a href="datahandling.html#regex"><i class="fa fa-check"></i><b>7.8.9</b> Regular expressions (not really)</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="datahandling.html"><a href="datahandling.html#importing"><i class="fa fa-check"></i><b>7.9</b> Reading unusual data files</a><ul>
<li class="chapter" data-level="7.9.1" data-path="datahandling.html"><a href="datahandling.html#loading-data-from-text-files"><i class="fa fa-check"></i><b>7.9.1</b> Loading data from text files</a></li>
<li class="chapter" data-level="7.9.2" data-path="datahandling.html"><a href="datahandling.html#loading-data-from-spss-and-other-statistics-packages"><i class="fa fa-check"></i><b>7.9.2</b> Loading data from SPSS (and other statistics packages)</a></li>
<li class="chapter" data-level="7.9.3" data-path="datahandling.html"><a href="datahandling.html#loading-excel-files"><i class="fa fa-check"></i><b>7.9.3</b> Loading Excel files</a></li>
<li class="chapter" data-level="7.9.4" data-path="datahandling.html"><a href="datahandling.html#loading-matlab-octave-files"><i class="fa fa-check"></i><b>7.9.4</b> Loading Matlab (&amp; Octave) files</a></li>
<li class="chapter" data-level="7.9.5" data-path="datahandling.html"><a href="datahandling.html#saving-other-kinds-of-data"><i class="fa fa-check"></i><b>7.9.5</b> Saving other kinds of data</a></li>
<li class="chapter" data-level="7.9.6" data-path="datahandling.html"><a href="datahandling.html#are-we-done-yet"><i class="fa fa-check"></i><b>7.9.6</b> Are we done yet?</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="datahandling.html"><a href="datahandling.html#coercion"><i class="fa fa-check"></i><b>7.10</b> Coercing data from one class to another</a></li>
<li class="chapter" data-level="7.11" data-path="datahandling.html"><a href="datahandling.html#datastructures"><i class="fa fa-check"></i><b>7.11</b> Other useful data structures</a><ul>
<li class="chapter" data-level="7.11.1" data-path="datahandling.html"><a href="datahandling.html#matrix"><i class="fa fa-check"></i><b>7.11.1</b> Matrices</a></li>
<li class="chapter" data-level="7.11.2" data-path="datahandling.html"><a href="datahandling.html#orderedfactors"><i class="fa fa-check"></i><b>7.11.2</b> Ordered factors</a></li>
<li class="chapter" data-level="7.11.3" data-path="datahandling.html"><a href="datahandling.html#dates"><i class="fa fa-check"></i><b>7.11.3</b> Dates and times</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="datahandling.html"><a href="datahandling.html#miscdatahandling"><i class="fa fa-check"></i><b>7.12</b> Miscellaneous topics</a><ul>
<li class="chapter" data-level="7.12.1" data-path="datahandling.html"><a href="datahandling.html#the-problems-with-floating-point-arithmetic"><i class="fa fa-check"></i><b>7.12.1</b> The problems with floating point arithmetic</a></li>
<li class="chapter" data-level="7.12.2" data-path="datahandling.html"><a href="datahandling.html#recycling"><i class="fa fa-check"></i><b>7.12.2</b> The recycling rule</a></li>
<li class="chapter" data-level="7.12.3" data-path="datahandling.html"><a href="datahandling.html#environments"><i class="fa fa-check"></i><b>7.12.3</b> An introduction to environments</a></li>
<li class="chapter" data-level="7.12.4" data-path="datahandling.html"><a href="datahandling.html#attaching-a-data-frame"><i class="fa fa-check"></i><b>7.12.4</b> Attaching a data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="datahandling.html"><a href="datahandling.html#summary-5"><i class="fa fa-check"></i><b>7.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="scripting.html"><a href="scripting.html"><i class="fa fa-check"></i><b>8</b> Basic programming</a><ul>
<li class="chapter" data-level="8.1" data-path="scripting.html"><a href="scripting.html#scripts"><i class="fa fa-check"></i><b>8.1</b> Scripts</a><ul>
<li class="chapter" data-level="8.1.1" data-path="scripting.html"><a href="scripting.html#why-use-scripts"><i class="fa fa-check"></i><b>8.1.1</b> Why use scripts?</a></li>
<li class="chapter" data-level="8.1.2" data-path="scripting.html"><a href="scripting.html#our-first-script"><i class="fa fa-check"></i><b>8.1.2</b> Our first script</a></li>
<li class="chapter" data-level="8.1.3" data-path="scripting.html"><a href="scripting.html#using-rstudio-to-write-scripts"><i class="fa fa-check"></i><b>8.1.3</b> Using Rstudio to write scripts</a></li>
<li class="chapter" data-level="8.1.4" data-path="scripting.html"><a href="scripting.html#commenting-your-script"><i class="fa fa-check"></i><b>8.1.4</b> Commenting your script</a></li>
<li class="chapter" data-level="8.1.5" data-path="scripting.html"><a href="scripting.html#differences-between-scripts-and-the-command-line"><i class="fa fa-check"></i><b>8.1.5</b> Differences between scripts and the command line</a></li>
<li class="chapter" data-level="8.1.6" data-path="scripting.html"><a href="scripting.html#done"><i class="fa fa-check"></i><b>8.1.6</b> Done!</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="scripting.html"><a href="scripting.html#loops"><i class="fa fa-check"></i><b>8.2</b> Loops</a><ul>
<li class="chapter" data-level="8.2.1" data-path="scripting.html"><a href="scripting.html#the-while-loop"><i class="fa fa-check"></i><b>8.2.1</b> The <code>while</code> loop</a></li>
<li class="chapter" data-level="8.2.2" data-path="scripting.html"><a href="scripting.html#for"><i class="fa fa-check"></i><b>8.2.2</b> The <code>for</code> loop</a></li>
<li class="chapter" data-level="8.2.3" data-path="scripting.html"><a href="scripting.html#a-more-realistic-example-of-a-loop"><i class="fa fa-check"></i><b>8.2.3</b> A more realistic example of a loop</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="scripting.html"><a href="scripting.html#if"><i class="fa fa-check"></i><b>8.3</b> Conditional statements</a></li>
<li class="chapter" data-level="8.4" data-path="scripting.html"><a href="scripting.html#functions"><i class="fa fa-check"></i><b>8.4</b> Writing functions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="scripting.html"><a href="scripting.html#dotsargument"><i class="fa fa-check"></i><b>8.4.1</b> Function arguments revisited</a></li>
<li class="chapter" data-level="8.4.2" data-path="scripting.html"><a href="scripting.html#theres-more-to-functions-than-this"><i class="fa fa-check"></i><b>8.4.2</b> There’s more to functions than this</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="scripting.html"><a href="scripting.html#vectorised"><i class="fa fa-check"></i><b>8.5</b> Implicit loops</a></li>
<li class="chapter" data-level="8.6" data-path="scripting.html"><a href="scripting.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iv-statistical-theory.html"><a href="part-iv-statistical-theory.html"><i class="fa fa-check"></i>Part IV. Statistical theory</a><ul>
<li class="chapter" data-level="" data-path="part-iv-statistical-theory.html"><a href="part-iv-statistical-theory.html#on-the-limits-of-logical-reasoning"><i class="fa fa-check"></i>On the limits of logical reasoning</a></li>
<li class="chapter" data-level="" data-path="part-iv-statistical-theory.html"><a href="part-iv-statistical-theory.html#learning-without-making-assumptions-is-a-myth"><i class="fa fa-check"></i>Learning without making assumptions is a myth</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>9</b> Introduction to probability</a><ul>
<li class="chapter" data-level="9.1" data-path="probability.html"><a href="probability.html#probstats"><i class="fa fa-check"></i><b>9.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="9.2" data-path="probability.html"><a href="probability.html#probmeaning"><i class="fa fa-check"></i><b>9.2</b> What does probability mean?</a><ul>
<li class="chapter" data-level="9.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>9.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="9.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>9.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="9.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>9.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>9.3</b> Basic probability theory</a><ul>
<li class="chapter" data-level="9.3.1" data-path="probability.html"><a href="probability.html#introducing-probability-distributions"><i class="fa fa-check"></i><b>9.3.1</b> Introducing probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>9.4</b> The binomial distribution</a><ul>
<li class="chapter" data-level="9.4.1" data-path="probability.html"><a href="probability.html#introducing-the-binomial"><i class="fa fa-check"></i><b>9.4.1</b> Introducing the binomial</a></li>
<li class="chapter" data-level="9.4.2" data-path="probability.html"><a href="probability.html#working-with-the-binomial-distribution-in-r"><i class="fa fa-check"></i><b>9.4.2</b> Working with the binomial distribution in R</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>9.5</b> The normal distribution</a><ul>
<li class="chapter" data-level="9.5.1" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>9.5.1</b> Probability density</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>9.6</b> Other useful distributions</a></li>
<li class="chapter" data-level="9.7" data-path="probability.html"><a href="probability.html#summary-7"><i class="fa fa-check"></i><b>9.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>10</b> Estimating unknown quantities from a sample</a><ul>
<li class="chapter" data-level="10.1" data-path="estimation.html"><a href="estimation.html#srs"><i class="fa fa-check"></i><b>10.1</b> Samples, populations and sampling</a><ul>
<li class="chapter" data-level="10.1.1" data-path="estimation.html"><a href="estimation.html#pop"><i class="fa fa-check"></i><b>10.1.1</b> Defining a population</a></li>
<li class="chapter" data-level="10.1.2" data-path="estimation.html"><a href="estimation.html#simple-random-samples"><i class="fa fa-check"></i><b>10.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="10.1.3" data-path="estimation.html"><a href="estimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>10.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="10.1.4" data-path="estimation.html"><a href="estimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>10.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="10.1.5" data-path="estimation.html"><a href="estimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>10.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="estimation.html"><a href="estimation.html#lawlargenumbers"><i class="fa fa-check"></i><b>10.2</b> The law of large numbers</a></li>
<li class="chapter" data-level="10.3" data-path="estimation.html"><a href="estimation.html#samplesandclt"><i class="fa fa-check"></i><b>10.3</b> Sampling distributions and the central limit theorem</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimation.html"><a href="estimation.html#samplingdists"><i class="fa fa-check"></i><b>10.3.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="10.3.2" data-path="estimation.html"><a href="estimation.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>10.3.2</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="10.3.3" data-path="estimation.html"><a href="estimation.html#clt"><i class="fa fa-check"></i><b>10.3.3</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimation.html"><a href="estimation.html#pointestimates"><i class="fa fa-check"></i><b>10.4</b> Estimating population parameters</a><ul>
<li class="chapter" data-level="10.4.1" data-path="estimation.html"><a href="estimation.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>10.4.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="10.4.2" data-path="estimation.html"><a href="estimation.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>10.4.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="estimation.html"><a href="estimation.html#ci"><i class="fa fa-check"></i><b>10.5</b> Estimating a confidence interval</a><ul>
<li class="chapter" data-level="10.5.1" data-path="estimation.html"><a href="estimation.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>10.5.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="10.5.2" data-path="estimation.html"><a href="estimation.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>10.5.2</b> Interpreting a confidence interval</a></li>
<li class="chapter" data-level="10.5.3" data-path="estimation.html"><a href="estimation.html#calculating-confidence-intervals-in-r"><i class="fa fa-check"></i><b>10.5.3</b> Calculating confidence intervals in R</a></li>
<li class="chapter" data-level="10.5.4" data-path="estimation.html"><a href="estimation.html#ciplots"><i class="fa fa-check"></i><b>10.5.4</b> Plotting confidence intervals in R</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="estimation.html"><a href="estimation.html#summary-8"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>11</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="11.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>11.1</b> A menagerie of hypotheses</a><ul>
<li class="chapter" data-level="11.1.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>11.1.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="11.1.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>11.1.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>11.2</b> Two types of errors</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>11.3</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>11.4</b> Making decisions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>11.4.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="11.4.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>11.4.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="11.4.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>11.4.3</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>11.5</b> The <span class="math inline">\(p\)</span> value of a test</a><ul>
<li class="chapter" data-level="11.5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>11.5.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="11.5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>11.5.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="11.5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>11.5.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>11.6</b> Reporting the results of a hypothesis test</a><ul>
<li class="chapter" data-level="11.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>11.6.1</b> The issue</a></li>
<li class="chapter" data-level="11.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>11.6.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#running-the-hypothesis-test-in-practice"><i class="fa fa-check"></i><b>11.7</b> Running the hypothesis test in practice</a></li>
<li class="chapter" data-level="11.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>11.8</b> Effect size, sample size and power</a><ul>
<li class="chapter" data-level="11.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>11.8.1</b> The power function</a></li>
<li class="chapter" data-level="11.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>11.8.2</b> Effect size</a></li>
<li class="chapter" data-level="11.8.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>11.8.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>11.9</b> Some issues to consider</a><ul>
<li class="chapter" data-level="11.9.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>11.9.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="11.9.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>11.9.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="11.9.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>11.9.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-9"><i class="fa fa-check"></i><b>11.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-v-statistical-tools.html"><a href="part-v-statistical-tools.html"><i class="fa fa-check"></i>Part V. Statistical tools</a></li>
<li class="chapter" data-level="12" data-path="chisquare.html"><a href="chisquare.html"><i class="fa fa-check"></i><b>12</b> Categorical data analysis</a><ul>
<li class="chapter" data-level="12.1" data-path="chisquare.html"><a href="chisquare.html#goftest"><i class="fa fa-check"></i><b>12.1</b> The <span class="math inline">\(\chi^2\)</span> goodness-of-fit test</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chisquare.html"><a href="chisquare.html#the-cards-data"><i class="fa fa-check"></i><b>12.1.1</b> The cards data</a></li>
<li class="chapter" data-level="12.1.2" data-path="chisquare.html"><a href="chisquare.html#the-null-hypothesis-and-the-alternative-hypothesis"><i class="fa fa-check"></i><b>12.1.2</b> The null hypothesis and the alternative hypothesis</a></li>
<li class="chapter" data-level="12.1.3" data-path="chisquare.html"><a href="chisquare.html#the-goodness-of-fit-test-statistic"><i class="fa fa-check"></i><b>12.1.3</b> The “goodness of fit” test statistic</a></li>
<li class="chapter" data-level="12.1.4" data-path="chisquare.html"><a href="chisquare.html#the-sampling-distribution-of-the-gof-statistic-advanced"><i class="fa fa-check"></i><b>12.1.4</b> The sampling distribution of the GOF statistic (advanced)</a></li>
<li class="chapter" data-level="12.1.5" data-path="chisquare.html"><a href="chisquare.html#degrees-of-freedom"><i class="fa fa-check"></i><b>12.1.5</b> Degrees of freedom</a></li>
<li class="chapter" data-level="12.1.6" data-path="chisquare.html"><a href="chisquare.html#testing-the-null-hypothesis"><i class="fa fa-check"></i><b>12.1.6</b> Testing the null hypothesis</a></li>
<li class="chapter" data-level="12.1.7" data-path="chisquare.html"><a href="chisquare.html#gofTestInR"><i class="fa fa-check"></i><b>12.1.7</b> Doing the test in R</a></li>
<li class="chapter" data-level="12.1.8" data-path="chisquare.html"><a href="chisquare.html#specifying-a-different-null-hypothesis"><i class="fa fa-check"></i><b>12.1.8</b> Specifying a different null hypothesis</a></li>
<li class="chapter" data-level="12.1.9" data-path="chisquare.html"><a href="chisquare.html#chisqreport"><i class="fa fa-check"></i><b>12.1.9</b> How to report the results of the test</a></li>
<li class="chapter" data-level="12.1.10" data-path="chisquare.html"><a href="chisquare.html#a-comment-on-statistical-notation-advanced"><i class="fa fa-check"></i><b>12.1.10</b> A comment on statistical notation (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chisquare.html"><a href="chisquare.html#chisqindependence"><i class="fa fa-check"></i><b>12.2</b> The <span class="math inline">\(\chi^2\)</span> test of independence (or association)</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chisquare.html"><a href="chisquare.html#constructing-our-hypothesis-test"><i class="fa fa-check"></i><b>12.2.1</b> Constructing our hypothesis test</a></li>
<li class="chapter" data-level="12.2.2" data-path="chisquare.html"><a href="chisquare.html#AssocTestInR"><i class="fa fa-check"></i><b>12.2.2</b> Doing the test in R</a></li>
<li class="chapter" data-level="12.2.3" data-path="chisquare.html"><a href="chisquare.html#postscript"><i class="fa fa-check"></i><b>12.2.3</b> Postscript</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chisquare.html"><a href="chisquare.html#yates"><i class="fa fa-check"></i><b>12.3</b> The continuity correction</a></li>
<li class="chapter" data-level="12.4" data-path="chisquare.html"><a href="chisquare.html#chisqeffectsize"><i class="fa fa-check"></i><b>12.4</b> Effect size</a></li>
<li class="chapter" data-level="12.5" data-path="chisquare.html"><a href="chisquare.html#chisqassumptions"><i class="fa fa-check"></i><b>12.5</b> Assumptions of the test(s)</a></li>
<li class="chapter" data-level="12.6" data-path="chisquare.html"><a href="chisquare.html#chisq.test"><i class="fa fa-check"></i><b>12.6</b> The most typical way to do chi-square tests in R</a></li>
<li class="chapter" data-level="12.7" data-path="chisquare.html"><a href="chisquare.html#fisherexacttest"><i class="fa fa-check"></i><b>12.7</b> The Fisher exact test</a></li>
<li class="chapter" data-level="12.8" data-path="chisquare.html"><a href="chisquare.html#mcnemar"><i class="fa fa-check"></i><b>12.8</b> The McNemar test</a><ul>
<li class="chapter" data-level="12.8.1" data-path="chisquare.html"><a href="chisquare.html#doing-the-mcnemar-test-in-r"><i class="fa fa-check"></i><b>12.8.1</b> Doing the McNemar test in R</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="chisquare.html"><a href="chisquare.html#whats-the-difference-between-mcnemar-and-independence"><i class="fa fa-check"></i><b>12.9</b> What’s the difference between McNemar and independence?</a></li>
<li class="chapter" data-level="12.10" data-path="chisquare.html"><a href="chisquare.html#summary-10"><i class="fa fa-check"></i><b>12.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>13</b> Comparing two means</a><ul>
<li class="chapter" data-level="13.1" data-path="ttest.html"><a href="ttest.html#the-one-sample-z-test"><i class="fa fa-check"></i><b>13.1</b> The one-sample <span class="math inline">\(z\)</span>-test</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ttest.html"><a href="ttest.html#the-inference-problem-that-the-test-addresses"><i class="fa fa-check"></i><b>13.1.1</b> The inference problem that the test addresses</a></li>
<li class="chapter" data-level="13.1.2" data-path="ttest.html"><a href="ttest.html#constructing-the-hypothesis-test"><i class="fa fa-check"></i><b>13.1.2</b> Constructing the hypothesis test</a></li>
<li class="chapter" data-level="13.1.3" data-path="ttest.html"><a href="ttest.html#a-worked-example-using-r"><i class="fa fa-check"></i><b>13.1.3</b> A worked example using R</a></li>
<li class="chapter" data-level="13.1.4" data-path="ttest.html"><a href="ttest.html#zassumptions"><i class="fa fa-check"></i><b>13.1.4</b> Assumptions of the <span class="math inline">\(z\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ttest.html"><a href="ttest.html#onesamplettest"><i class="fa fa-check"></i><b>13.2</b> The one-sample <span class="math inline">\(t\)</span>-test</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ttest.html"><a href="ttest.html#introducing-the-t-test"><i class="fa fa-check"></i><b>13.2.1</b> Introducing the <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="13.2.2" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r"><i class="fa fa-check"></i><b>13.2.2</b> Doing the test in R</a></li>
<li class="chapter" data-level="13.2.3" data-path="ttest.html"><a href="ttest.html#ttestoneassumptions"><i class="fa fa-check"></i><b>13.2.3</b> Assumptions of the one sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ttest.html"><a href="ttest.html#studentttest"><i class="fa fa-check"></i><b>13.3</b> The independent samples <span class="math inline">\(t\)</span>-test (Student test)</a><ul>
<li class="chapter" data-level="13.3.1" data-path="ttest.html"><a href="ttest.html#the-data-1"><i class="fa fa-check"></i><b>13.3.1</b> The data</a></li>
<li class="chapter" data-level="13.3.2" data-path="ttest.html"><a href="ttest.html#introducing-the-test"><i class="fa fa-check"></i><b>13.3.2</b> Introducing the test</a></li>
<li class="chapter" data-level="13.3.3" data-path="ttest.html"><a href="ttest.html#a-pooled-estimate-of-the-standard-deviation"><i class="fa fa-check"></i><b>13.3.3</b> A “pooled estimate” of the standard deviation</a></li>
<li class="chapter" data-level="13.3.4" data-path="ttest.html"><a href="ttest.html#the-same-pooled-estimate-described-differently"><i class="fa fa-check"></i><b>13.3.4</b> The same pooled estimate, described differently</a></li>
<li class="chapter" data-level="13.3.5" data-path="ttest.html"><a href="ttest.html#completing-the-test"><i class="fa fa-check"></i><b>13.3.5</b> Completing the test</a></li>
<li class="chapter" data-level="13.3.6" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-1"><i class="fa fa-check"></i><b>13.3.6</b> Doing the test in R</a></li>
<li class="chapter" data-level="13.3.7" data-path="ttest.html"><a href="ttest.html#positive-and-negative-t-values"><i class="fa fa-check"></i><b>13.3.7</b> Positive and negative <span class="math inline">\(t\)</span> values</a></li>
<li class="chapter" data-level="13.3.8" data-path="ttest.html"><a href="ttest.html#studentassumptions"><i class="fa fa-check"></i><b>13.3.8</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ttest.html"><a href="ttest.html#welchttest"><i class="fa fa-check"></i><b>13.4</b> The independent samples <span class="math inline">\(t\)</span>-test (Welch test)</a><ul>
<li class="chapter" data-level="13.4.1" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-2"><i class="fa fa-check"></i><b>13.4.1</b> Doing the test in R</a></li>
<li class="chapter" data-level="13.4.2" data-path="ttest.html"><a href="ttest.html#assumptions-of-the-test"><i class="fa fa-check"></i><b>13.4.2</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ttest.html"><a href="ttest.html#pairedsamplesttest"><i class="fa fa-check"></i><b>13.5</b> The paired-samples <span class="math inline">\(t\)</span>-test</a><ul>
<li class="chapter" data-level="13.5.1" data-path="ttest.html"><a href="ttest.html#the-data-2"><i class="fa fa-check"></i><b>13.5.1</b> The data</a></li>
<li class="chapter" data-level="13.5.2" data-path="ttest.html"><a href="ttest.html#what-is-the-paired-samples-t-test"><i class="fa fa-check"></i><b>13.5.2</b> What is the paired samples <span class="math inline">\(t\)</span>-test?</a></li>
<li class="chapter" data-level="13.5.3" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-1"><i class="fa fa-check"></i><b>13.5.3</b> Doing the test in R, part 1</a></li>
<li class="chapter" data-level="13.5.4" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-2"><i class="fa fa-check"></i><b>13.5.4</b> Doing the test in R, part 2</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ttest.html"><a href="ttest.html#one-sided-tests"><i class="fa fa-check"></i><b>13.6</b> One sided tests</a></li>
<li class="chapter" data-level="13.7" data-path="ttest.html"><a href="ttest.html#ttestfunction"><i class="fa fa-check"></i><b>13.7</b> Using the t.test() function</a></li>
<li class="chapter" data-level="13.8" data-path="ttest.html"><a href="ttest.html#cohensd"><i class="fa fa-check"></i><b>13.8</b> Effect size</a><ul>
<li class="chapter" data-level="13.8.1" data-path="ttest.html"><a href="ttest.html#cohens-d-from-one-sample"><i class="fa fa-check"></i><b>13.8.1</b> Cohen’s <span class="math inline">\(d\)</span> from one sample</a></li>
<li class="chapter" data-level="13.8.2" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-student-t-test"><i class="fa fa-check"></i><b>13.8.2</b> Cohen’s <span class="math inline">\(d\)</span> from a Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="13.8.3" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-welch-test"><i class="fa fa-check"></i><b>13.8.3</b> Cohen’s <span class="math inline">\(d\)</span> from a Welch test</a></li>
<li class="chapter" data-level="13.8.4" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-paired-samples-test"><i class="fa fa-check"></i><b>13.8.4</b> Cohen’s <span class="math inline">\(d\)</span> from a paired-samples test</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="ttest.html"><a href="ttest.html#shapiro"><i class="fa fa-check"></i><b>13.9</b> Checking the normality of a sample</a><ul>
<li class="chapter" data-level="13.9.1" data-path="ttest.html"><a href="ttest.html#qq-plots"><i class="fa fa-check"></i><b>13.9.1</b> QQ plots</a></li>
<li class="chapter" data-level="13.9.2" data-path="ttest.html"><a href="ttest.html#shapiro-wilk-tests"><i class="fa fa-check"></i><b>13.9.2</b> Shapiro-Wilk tests</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="ttest.html"><a href="ttest.html#wilcox"><i class="fa fa-check"></i><b>13.10</b> Testing non-normal data with Wilcoxon tests</a><ul>
<li class="chapter" data-level="13.10.1" data-path="ttest.html"><a href="ttest.html#two-sample-wilcoxon-test"><i class="fa fa-check"></i><b>13.10.1</b> Two sample Wilcoxon test</a></li>
<li class="chapter" data-level="13.10.2" data-path="ttest.html"><a href="ttest.html#one-sample-wilcoxon-test"><i class="fa fa-check"></i><b>13.10.2</b> One sample Wilcoxon test</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="ttest.html"><a href="ttest.html#summary-11"><i class="fa fa-check"></i><b>13.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>14</b> Comparing several means (one-way ANOVA)</a><ul>
<li class="chapter" data-level="14.1" data-path="anova.html"><a href="anova.html#anxifree"><i class="fa fa-check"></i><b>14.1</b> An illustrative data set</a></li>
<li class="chapter" data-level="14.2" data-path="anova.html"><a href="anova.html#anovaintro"><i class="fa fa-check"></i><b>14.2</b> How ANOVA works</a><ul>
<li class="chapter" data-level="14.2.1" data-path="anova.html"><a href="anova.html#two-formulas-for-the-variance-of-y"><i class="fa fa-check"></i><b>14.2.1</b> Two formulas for the variance of <span class="math inline">\(Y\)</span></a></li>
<li class="chapter" data-level="14.2.2" data-path="anova.html"><a href="anova.html#from-variances-to-sums-of-squares"><i class="fa fa-check"></i><b>14.2.2</b> From variances to sums of squares</a></li>
<li class="chapter" data-level="14.2.3" data-path="anova.html"><a href="anova.html#from-sums-of-squares-to-the-f-test"><i class="fa fa-check"></i><b>14.2.3</b> From sums of squares to the <span class="math inline">\(F\)</span>-test</a></li>
<li class="chapter" data-level="14.2.4" data-path="anova.html"><a href="anova.html#anovamodel"><i class="fa fa-check"></i><b>14.2.4</b> The model for the data and the meaning of <span class="math inline">\(F\)</span> (advanced)</a></li>
<li class="chapter" data-level="14.2.5" data-path="anova.html"><a href="anova.html#anovacalc"><i class="fa fa-check"></i><b>14.2.5</b> A worked example</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="anova.html"><a href="anova.html#introduceaov"><i class="fa fa-check"></i><b>14.3</b> Running an ANOVA in R</a><ul>
<li class="chapter" data-level="14.3.1" data-path="anova.html"><a href="anova.html#using-the-aov-function-to-specify-your-anova"><i class="fa fa-check"></i><b>14.3.1</b> Using the <code>aov()</code> function to specify your ANOVA</a></li>
<li class="chapter" data-level="14.3.2" data-path="anova.html"><a href="anova.html#aovobjects"><i class="fa fa-check"></i><b>14.3.2</b> Understanding what the <code>aov()</code> function produces</a></li>
<li class="chapter" data-level="14.3.3" data-path="anova.html"><a href="anova.html#running-the-hypothesis-tests-for-the-anova"><i class="fa fa-check"></i><b>14.3.3</b> Running the hypothesis tests for the ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="anova.html"><a href="anova.html#etasquared"><i class="fa fa-check"></i><b>14.4</b> Effect size</a></li>
<li class="chapter" data-level="14.5" data-path="anova.html"><a href="anova.html#posthoc"><i class="fa fa-check"></i><b>14.5</b> Multiple comparisons and post hoc tests</a><ul>
<li class="chapter" data-level="14.5.1" data-path="anova.html"><a href="anova.html#running-pairwise-t-tests"><i class="fa fa-check"></i><b>14.5.1</b> Running “pairwise” <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="14.5.2" data-path="anova.html"><a href="anova.html#corrections-for-multiple-testing"><i class="fa fa-check"></i><b>14.5.2</b> Corrections for multiple testing</a></li>
<li class="chapter" data-level="14.5.3" data-path="anova.html"><a href="anova.html#bonferroni-corrections"><i class="fa fa-check"></i><b>14.5.3</b> Bonferroni corrections</a></li>
<li class="chapter" data-level="14.5.4" data-path="anova.html"><a href="anova.html#holm-corrections"><i class="fa fa-check"></i><b>14.5.4</b> Holm corrections</a></li>
<li class="chapter" data-level="14.5.5" data-path="anova.html"><a href="anova.html#writing-up-the-post-hoc-test"><i class="fa fa-check"></i><b>14.5.5</b> Writing up the post hoc test</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="anova.html"><a href="anova.html#anovaassumptions"><i class="fa fa-check"></i><b>14.6</b> Assumptions of one-way ANOVA</a><ul>
<li class="chapter" data-level="14.6.1" data-path="anova.html"><a href="anova.html#how-robust-is-anova"><i class="fa fa-check"></i><b>14.6.1</b> How robust is ANOVA?</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="anova.html"><a href="anova.html#levene"><i class="fa fa-check"></i><b>14.7</b> Checking the homogeneity of variance assumption</a><ul>
<li class="chapter" data-level="14.7.1" data-path="anova.html"><a href="anova.html#running-the-levenes-test-in-r"><i class="fa fa-check"></i><b>14.7.1</b> Running the Levene’s test in R</a></li>
<li class="chapter" data-level="14.7.2" data-path="anova.html"><a href="anova.html#additional-comments"><i class="fa fa-check"></i><b>14.7.2</b> Additional comments</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="anova.html"><a href="anova.html#welchoneway"><i class="fa fa-check"></i><b>14.8</b> Removing the homogeneity of variance assumption</a></li>
<li class="chapter" data-level="14.9" data-path="anova.html"><a href="anova.html#anovanormality"><i class="fa fa-check"></i><b>14.9</b> Checking the normality assumption</a></li>
<li class="chapter" data-level="14.10" data-path="anova.html"><a href="anova.html#kruskalwallis"><i class="fa fa-check"></i><b>14.10</b> Removing the normality assumption</a><ul>
<li class="chapter" data-level="14.10.1" data-path="anova.html"><a href="anova.html#the-logic-behind-the-kruskal-wallis-test"><i class="fa fa-check"></i><b>14.10.1</b> The logic behind the Kruskal-Wallis test</a></li>
<li class="chapter" data-level="14.10.2" data-path="anova.html"><a href="anova.html#additional-details"><i class="fa fa-check"></i><b>14.10.2</b> Additional details</a></li>
<li class="chapter" data-level="14.10.3" data-path="anova.html"><a href="anova.html#how-to-run-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>14.10.3</b> How to run the Kruskal-Wallis test in R</a></li>
</ul></li>
<li class="chapter" data-level="14.11" data-path="anova.html"><a href="anova.html#anovaandt"><i class="fa fa-check"></i><b>14.11</b> On the relationship between ANOVA and the Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="14.12" data-path="anova.html"><a href="anova.html#summary-12"><i class="fa fa-check"></i><b>14.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>15</b> Linear regression</a><ul>
<li class="chapter" data-level="15.1" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>15.1</b> What is a linear regression model?</a></li>
<li class="chapter" data-level="15.2" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>15.2</b> Estimating a linear regression model</a><ul>
<li class="chapter" data-level="15.2.1" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>15.2.1</b> Using the <code>lm()</code> function</a></li>
<li class="chapter" data-level="15.2.2" data-path="regression.html"><a href="regression.html#interpreting-the-estimated-model"><i class="fa fa-check"></i><b>15.2.2</b> Interpreting the estimated model</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="regression.html"><a href="regression.html#multipleregression"><i class="fa fa-check"></i><b>15.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="15.3.1" data-path="regression.html"><a href="regression.html#doing-it-in-r"><i class="fa fa-check"></i><b>15.3.1</b> Doing it in R</a></li>
<li class="chapter" data-level="15.3.2" data-path="regression.html"><a href="regression.html#formula-for-the-general-case"><i class="fa fa-check"></i><b>15.3.2</b> Formula for the general case</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>15.4</b> Quantifying the fit of the regression model</a><ul>
<li class="chapter" data-level="15.4.1" data-path="regression.html"><a href="regression.html#the-r2-value"><i class="fa fa-check"></i><b>15.4.1</b> The <span class="math inline">\(R^2\)</span> value</a></li>
<li class="chapter" data-level="15.4.2" data-path="regression.html"><a href="regression.html#the-relationship-between-regression-and-correlation"><i class="fa fa-check"></i><b>15.4.2</b> The relationship between regression and correlation</a></li>
<li class="chapter" data-level="15.4.3" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>15.4.3</b> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>15.5</b> Hypothesis tests for regression models</a><ul>
<li class="chapter" data-level="15.5.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole"><i class="fa fa-check"></i><b>15.5.1</b> Testing the model as a whole</a></li>
<li class="chapter" data-level="15.5.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>15.5.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="15.5.3" data-path="regression.html"><a href="regression.html#regressionsummary"><i class="fa fa-check"></i><b>15.5.3</b> Running the hypothesis tests in R</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="regression.html"><a href="regression.html#corrhyp"><i class="fa fa-check"></i><b>15.6</b> Testing the significance of a correlation</a><ul>
<li class="chapter" data-level="15.6.1" data-path="regression.html"><a href="regression.html#hypothesis-tests-for-a-single-correlation"><i class="fa fa-check"></i><b>15.6.1</b> Hypothesis tests for a single correlation</a></li>
<li class="chapter" data-level="15.6.2" data-path="regression.html"><a href="regression.html#corrhyp2"><i class="fa fa-check"></i><b>15.6.2</b> Hypothesis tests for all pairwise correlations</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="regression.html"><a href="regression.html#regressioncoefs"><i class="fa fa-check"></i><b>15.7</b> Regarding regression coefficients</a><ul>
<li class="chapter" data-level="15.7.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>15.7.1</b> Confidence intervals for the coefficients</a></li>
<li class="chapter" data-level="15.7.2" data-path="regression.html"><a href="regression.html#calculating-standardised-regression-coefficients"><i class="fa fa-check"></i><b>15.7.2</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>15.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="15.9" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>15.9</b> Model checking</a><ul>
<li class="chapter" data-level="15.9.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>15.9.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="15.9.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>15.9.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="15.9.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>15.9.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="15.9.4" data-path="regression.html"><a href="regression.html#regressionlinearity"><i class="fa fa-check"></i><b>15.9.4</b> Checking the linearity of the relationship</a></li>
<li class="chapter" data-level="15.9.5" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>15.9.5</b> Checking the homogeneity of variance</a></li>
<li class="chapter" data-level="15.9.6" data-path="regression.html"><a href="regression.html#regressioncollinearity"><i class="fa fa-check"></i><b>15.9.6</b> Checking for collinearity</a></li>
</ul></li>
<li class="chapter" data-level="15.10" data-path="regression.html"><a href="regression.html#modelselreg"><i class="fa fa-check"></i><b>15.10</b> Model selection</a><ul>
<li class="chapter" data-level="15.10.1" data-path="regression.html"><a href="regression.html#backward-elimination"><i class="fa fa-check"></i><b>15.10.1</b> Backward elimination</a></li>
<li class="chapter" data-level="15.10.2" data-path="regression.html"><a href="regression.html#forward-selection"><i class="fa fa-check"></i><b>15.10.2</b> Forward selection</a></li>
<li class="chapter" data-level="15.10.3" data-path="regression.html"><a href="regression.html#a-caveat"><i class="fa fa-check"></i><b>15.10.3</b> A caveat</a></li>
<li class="chapter" data-level="15.10.4" data-path="regression.html"><a href="regression.html#comparing-two-regression-models"><i class="fa fa-check"></i><b>15.10.4</b> Comparing two regression models</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="regression.html"><a href="regression.html#summary-13"><i class="fa fa-check"></i><b>15.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="anova2.html"><a href="anova2.html"><i class="fa fa-check"></i><b>16</b> Factorial ANOVA</a><ul>
<li class="chapter" data-level="16.1" data-path="anova2.html"><a href="anova2.html#factorialanovasimple"><i class="fa fa-check"></i><b>16.1</b> Factorial ANOVA 1: balanced designs, no interactions</a><ul>
<li class="chapter" data-level="16.1.1" data-path="anova2.html"><a href="anova2.html#factanovahyp"><i class="fa fa-check"></i><b>16.1.1</b> What hypotheses are we testing?</a></li>
<li class="chapter" data-level="16.1.2" data-path="anova2.html"><a href="anova2.html#running-the-analysis-in-r"><i class="fa fa-check"></i><b>16.1.2</b> Running the analysis in R</a></li>
<li class="chapter" data-level="16.1.3" data-path="anova2.html"><a href="anova2.html#how-are-the-sum-of-squares-calculated"><i class="fa fa-check"></i><b>16.1.3</b> How are the sum of squares calculated?</a></li>
<li class="chapter" data-level="16.1.4" data-path="anova2.html"><a href="anova2.html#what-are-our-degrees-of-freedom"><i class="fa fa-check"></i><b>16.1.4</b> What are our degrees of freedom?</a></li>
<li class="chapter" data-level="16.1.5" data-path="anova2.html"><a href="anova2.html#factorial-anova-versus-one-way-anovas"><i class="fa fa-check"></i><b>16.1.5</b> Factorial ANOVA versus one-way ANOVAs</a></li>
<li class="chapter" data-level="16.1.6" data-path="anova2.html"><a href="anova2.html#what-kinds-of-outcomes-does-this-analysis-capture"><i class="fa fa-check"></i><b>16.1.6</b> What kinds of outcomes does this analysis capture?</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="anova2.html"><a href="anova2.html#interactions"><i class="fa fa-check"></i><b>16.2</b> Factorial ANOVA 2: balanced designs, interactions allowed</a><ul>
<li class="chapter" data-level="16.2.1" data-path="anova2.html"><a href="anova2.html#what-exactly-is-an-interaction-effect"><i class="fa fa-check"></i><b>16.2.1</b> What exactly <em>is an interaction effect?</em></a></li>
<li class="chapter" data-level="16.2.2" data-path="anova2.html"><a href="anova2.html#calculating-sums-of-squares-for-the-interaction"><i class="fa fa-check"></i><b>16.2.2</b> Calculating sums of squares for the interaction</a></li>
<li class="chapter" data-level="16.2.3" data-path="anova2.html"><a href="anova2.html#degrees-of-freedom-for-the-interaction"><i class="fa fa-check"></i><b>16.2.3</b> Degrees of freedom for the interaction</a></li>
<li class="chapter" data-level="16.2.4" data-path="anova2.html"><a href="anova2.html#running-the-anova-in-r"><i class="fa fa-check"></i><b>16.2.4</b> Running the ANOVA in R</a></li>
<li class="chapter" data-level="16.2.5" data-path="anova2.html"><a href="anova2.html#interpreting-the-results"><i class="fa fa-check"></i><b>16.2.5</b> Interpreting the results</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="anova2.html"><a href="anova2.html#effectsizefactorialanova"><i class="fa fa-check"></i><b>16.3</b> Effect size, estimated means, and confidence intervals</a><ul>
<li class="chapter" data-level="16.3.1" data-path="anova2.html"><a href="anova2.html#effect-sizes"><i class="fa fa-check"></i><b>16.3.1</b> Effect sizes</a></li>
<li class="chapter" data-level="16.3.2" data-path="anova2.html"><a href="anova2.html#estimated-group-means"><i class="fa fa-check"></i><b>16.3.2</b> Estimated group means</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="anova2.html"><a href="anova2.html#factorialanovaassumptions"><i class="fa fa-check"></i><b>16.4</b> Assumption checking</a><ul>
<li class="chapter" data-level="16.4.1" data-path="anova2.html"><a href="anova2.html#levene-test-for-homogeneity-of-variance"><i class="fa fa-check"></i><b>16.4.1</b> Levene test for homogeneity of variance</a></li>
<li class="chapter" data-level="16.4.2" data-path="anova2.html"><a href="anova2.html#normality-of-residuals"><i class="fa fa-check"></i><b>16.4.2</b> Normality of residuals</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="anova2.html"><a href="anova2.html#omnibusF"><i class="fa fa-check"></i><b>16.5</b> The <span class="math inline">\(F\)</span> test as a model comparison</a><ul>
<li class="chapter" data-level="16.5.1" data-path="anova2.html"><a href="anova2.html#the-f-test-comparing-two-models"><i class="fa fa-check"></i><b>16.5.1</b> The <span class="math inline">\(F\)</span> test comparing two models</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="anova2.html"><a href="anova2.html#anovalm"><i class="fa fa-check"></i><b>16.6</b> ANOVA as a linear model</a><ul>
<li class="chapter" data-level="16.6.1" data-path="anova2.html"><a href="anova2.html#some-data"><i class="fa fa-check"></i><b>16.6.1</b> Some data</a></li>
<li class="chapter" data-level="16.6.2" data-path="anova2.html"><a href="anova2.html#anova-with-binary-factors-as-a-regression-model"><i class="fa fa-check"></i><b>16.6.2</b> ANOVA with binary factors as a regression model</a></li>
<li class="chapter" data-level="16.6.3" data-path="anova2.html"><a href="anova2.html#changingbaseline"><i class="fa fa-check"></i><b>16.6.3</b> Changing the baseline category</a></li>
<li class="chapter" data-level="16.6.4" data-path="anova2.html"><a href="anova2.html#how-to-encode-non-binary-factors-as-contrasts"><i class="fa fa-check"></i><b>16.6.4</b> How to encode non binary factors as contrasts</a></li>
<li class="chapter" data-level="16.6.5" data-path="anova2.html"><a href="anova2.html#the-equivalence-between-anova-and-regression-for-non-binary-factors"><i class="fa fa-check"></i><b>16.6.5</b> The equivalence between ANOVA and regression for non-binary factors</a></li>
<li class="chapter" data-level="16.6.6" data-path="anova2.html"><a href="anova2.html#degrees-of-freedom-as-parameter-counting"><i class="fa fa-check"></i><b>16.6.6</b> Degrees of freedom as parameter counting!</a></li>
<li class="chapter" data-level="16.6.7" data-path="anova2.html"><a href="anova2.html#a-postscript"><i class="fa fa-check"></i><b>16.6.7</b> A postscript</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="anova2.html"><a href="anova2.html#contrasts"><i class="fa fa-check"></i><b>16.7</b> Different ways to specify contrasts</a><ul>
<li class="chapter" data-level="16.7.1" data-path="anova2.html"><a href="anova2.html#treatment-contrasts"><i class="fa fa-check"></i><b>16.7.1</b> Treatment contrasts</a></li>
<li class="chapter" data-level="16.7.2" data-path="anova2.html"><a href="anova2.html#helmert-contrasts"><i class="fa fa-check"></i><b>16.7.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="16.7.3" data-path="anova2.html"><a href="anova2.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>16.7.3</b> Sum to zero contrasts</a></li>
<li class="chapter" data-level="16.7.4" data-path="anova2.html"><a href="anova2.html#viewing-and-setting-the-default-contrasts-in-r"><i class="fa fa-check"></i><b>16.7.4</b> Viewing and setting the default contrasts in R</a></li>
<li class="chapter" data-level="16.7.5" data-path="anova2.html"><a href="anova2.html#setting-the-contrasts-for-a-single-factor"><i class="fa fa-check"></i><b>16.7.5</b> Setting the contrasts for a single factor</a></li>
<li class="chapter" data-level="16.7.6" data-path="anova2.html"><a href="anova2.html#setting-the-contrasts-for-a-single-analysis"><i class="fa fa-check"></i><b>16.7.6</b> Setting the contrasts for a single analysis</a></li>
</ul></li>
<li class="chapter" data-level="16.8" data-path="anova2.html"><a href="anova2.html#posthoc2"><i class="fa fa-check"></i><b>16.8</b> Post hoc tests</a></li>
<li class="chapter" data-level="16.9" data-path="anova2.html"><a href="anova2.html#plannedcomparisons"><i class="fa fa-check"></i><b>16.9</b> The method of planned comparisons</a></li>
<li class="chapter" data-level="16.10" data-path="anova2.html"><a href="anova2.html#unbalancedanova"><i class="fa fa-check"></i><b>16.10</b> Factorial ANOVA 3: unbalanced designs</a><ul>
<li class="chapter" data-level="16.10.1" data-path="anova2.html"><a href="anova2.html#the-coffee-data"><i class="fa fa-check"></i><b>16.10.1</b> The coffee data</a></li>
<li class="chapter" data-level="16.10.2" data-path="anova2.html"><a href="anova2.html#standard-anova-does-not-exist-for-unbalanced-designs"><i class="fa fa-check"></i><b>16.10.2</b> “Standard ANOVA” does not exist for unbalanced designs</a></li>
<li class="chapter" data-level="16.10.3" data-path="anova2.html"><a href="anova2.html#type-i-sum-of-squares"><i class="fa fa-check"></i><b>16.10.3</b> Type I sum of squares</a></li>
<li class="chapter" data-level="16.10.4" data-path="anova2.html"><a href="anova2.html#type-iii-sum-of-squares"><i class="fa fa-check"></i><b>16.10.4</b> Type III sum of squares</a></li>
<li class="chapter" data-level="16.10.5" data-path="anova2.html"><a href="anova2.html#type-ii-sum-of-squares"><i class="fa fa-check"></i><b>16.10.5</b> Type II sum of squares</a></li>
<li class="chapter" data-level="16.10.6" data-path="anova2.html"><a href="anova2.html#effect-sizes-and-non-additive-sums-of-squares"><i class="fa fa-check"></i><b>16.10.6</b> Effect sizes (and non-additive sums of squares)</a></li>
</ul></li>
<li class="chapter" data-level="16.11" data-path="anova2.html"><a href="anova2.html#summary-14"><i class="fa fa-check"></i><b>16.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-vi-endings-alternatives-and-prospects.html"><a href="part-vi-endings-alternatives-and-prospects.html"><i class="fa fa-check"></i>Part VI. Endings, alternatives and prospects</a></li>
<li class="chapter" data-level="17" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>17</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="bayes.html"><a href="bayes.html#basicbayes"><i class="fa fa-check"></i><b>17.1</b> Probabilistic reasoning by rational agents</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayes.html"><a href="bayes.html#priors-what-you-believed-before"><i class="fa fa-check"></i><b>17.1.1</b> Priors: what you believed before</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayes.html"><a href="bayes.html#likelihoods-theories-about-the-data"><i class="fa fa-check"></i><b>17.1.2</b> Likelihoods: theories about the data</a></li>
<li class="chapter" data-level="17.1.3" data-path="bayes.html"><a href="bayes.html#the-joint-probability-of-data-and-hypothesis"><i class="fa fa-check"></i><b>17.1.3</b> The joint probability of data and hypothesis</a></li>
<li class="chapter" data-level="17.1.4" data-path="bayes.html"><a href="bayes.html#updating-beliefs-using-bayes-rule"><i class="fa fa-check"></i><b>17.1.4</b> Updating beliefs using Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="bayes.html"><a href="bayes.html#bayesianhypothesistests"><i class="fa fa-check"></i><b>17.2</b> Bayesian hypothesis tests</a><ul>
<li class="chapter" data-level="17.2.1" data-path="bayes.html"><a href="bayes.html#the-bayes-factor"><i class="fa fa-check"></i><b>17.2.1</b> The Bayes factor</a></li>
<li class="chapter" data-level="17.2.2" data-path="bayes.html"><a href="bayes.html#interpreting-bayes-factors"><i class="fa fa-check"></i><b>17.2.2</b> Interpreting Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="bayes.html"><a href="bayes.html#whybayes"><i class="fa fa-check"></i><b>17.3</b> Why be a Bayesian?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="bayes.html"><a href="bayes.html#statistics-that-mean-what-you-think-they-mean"><i class="fa fa-check"></i><b>17.3.1</b> Statistics that mean what you think they mean</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="bayes.html"><a href="bayes.html#evidentiary-standards-you-can-believe"><i class="fa fa-check"></i><b>17.4</b> Evidentiary standards you can believe</a></li>
<li class="chapter" data-level="17.5" data-path="bayes.html"><a href="bayes.html#the-p-value-is-a-lie."><i class="fa fa-check"></i><b>17.5</b> The <span class="math inline">\(p\)</span>-value is a lie.</a><ul>
<li class="chapter" data-level="17.5.1" data-path="bayes.html"><a href="bayes.html#is-it-really-this-bad"><i class="fa fa-check"></i><b>17.5.1</b> Is it really this bad?</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="bayes.html"><a href="bayes.html#bayescontingency"><i class="fa fa-check"></i><b>17.6</b> Bayesian analysis of contingency tables</a><ul>
<li class="chapter" data-level="17.6.1" data-path="bayes.html"><a href="bayes.html#the-orthodox-text"><i class="fa fa-check"></i><b>17.6.1</b> The orthodox text</a></li>
<li class="chapter" data-level="17.6.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-test"><i class="fa fa-check"></i><b>17.6.2</b> The Bayesian test</a></li>
<li class="chapter" data-level="17.6.3" data-path="bayes.html"><a href="bayes.html#writing-up-the-results"><i class="fa fa-check"></i><b>17.6.3</b> Writing up the results</a></li>
<li class="chapter" data-level="17.6.4" data-path="bayes.html"><a href="bayes.html#other-sampling-plans"><i class="fa fa-check"></i><b>17.6.4</b> Other sampling plans</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="bayes.html"><a href="bayes.html#ttestbf"><i class="fa fa-check"></i><b>17.7</b> Bayesian <span class="math inline">\(t\)</span>-tests</a><ul>
<li class="chapter" data-level="17.7.1" data-path="bayes.html"><a href="bayes.html#independent-samples-t-test"><i class="fa fa-check"></i><b>17.7.1</b> Independent samples <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="17.7.2" data-path="bayes.html"><a href="bayes.html#paired-samples-t-test"><i class="fa fa-check"></i><b>17.7.2</b> Paired samples <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="bayes.html"><a href="bayes.html#bayesregression"><i class="fa fa-check"></i><b>17.8</b> Bayesian regression</a><ul>
<li class="chapter" data-level="17.8.1" data-path="bayes.html"><a href="bayes.html#a-quick-refresher"><i class="fa fa-check"></i><b>17.8.1</b> A quick refresher</a></li>
<li class="chapter" data-level="17.8.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-version"><i class="fa fa-check"></i><b>17.8.2</b> The Bayesian version</a></li>
<li class="chapter" data-level="17.8.3" data-path="bayes.html"><a href="bayes.html#finding-the-best-model"><i class="fa fa-check"></i><b>17.8.3</b> Finding the best model</a></li>
<li class="chapter" data-level="17.8.4" data-path="bayes.html"><a href="bayes.html#extracting-bayes-factors-for-all-included-terms"><i class="fa fa-check"></i><b>17.8.4</b> Extracting Bayes factors for all included terms</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="bayes.html"><a href="bayes.html#bayesanova"><i class="fa fa-check"></i><b>17.9</b> Bayesian ANOVA</a><ul>
<li class="chapter" data-level="17.9.1" data-path="bayes.html"><a href="bayes.html#a-quick-refresher-1"><i class="fa fa-check"></i><b>17.9.1</b> A quick refresher</a></li>
<li class="chapter" data-level="17.9.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-version-1"><i class="fa fa-check"></i><b>17.9.2</b> The Bayesian version</a></li>
<li class="chapter" data-level="17.9.3" data-path="bayes.html"><a href="bayes.html#constructing-bayesian-type-ii-tests"><i class="fa fa-check"></i><b>17.9.3</b> Constructing Bayesian Type II tests</a></li>
</ul></li>
<li class="chapter" data-level="17.10" data-path="bayes.html"><a href="bayes.html#summary-15"><i class="fa fa-check"></i><b>17.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i>Epilogue</a><ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#the-undiscovered-statistics"><i class="fa fa-check"></i>The undiscovered statistics</a><ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#omissions-within-the-topics-covered"><i class="fa fa-check"></i>Omissions within the topics covered</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#statistical-models-missing-from-the-book"><i class="fa fa-check"></i>Statistical models missing from the book</a></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#other-ways-of-doing-inference"><i class="fa fa-check"></i>Other ways of doing inference</a><ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#miscellaneous-topics"><i class="fa fa-check"></i>Miscellaneous topics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#learning-the-basics-and-learning-them-in-r"><i class="fa fa-check"></i>Learning the basics, and learning them in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://learningstatisticswithr.com/book/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chisquare" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Categorical data analysis</h1>
<p>Now that we’ve got the basic theory behind hypothesis testing, it’s time to start looking at specific tests that are commonly used in psychology. So where should we start? Not every textbook agrees on where to start, but I’m going to start with “<span class="math inline">\(\chi^2\)</span> tests” (this chapter) and “<span class="math inline">\(t\)</span>-tests” (Chapter <a href="ttest.html#ttest">13</a>). Both of these tools are very frequently used in scientific practice, and while they’re not as powerful as “analysis of variance” (Chapter <a href="anova.html#anova">14</a>) and “regression” (Chapter <a href="regression.html#regression">15</a>) they’re much easier to understand.</p>
<p>The term “categorical data” is just another name for “nominal scale data”. It’s nothing that we haven’t already discussed, it’s just that in the context of data analysis people tend to use the term “categorical data” rather than “nominal scale data”. I don’t know why. In any case, <strong><em>categorical data analysis</em></strong> refers to a collection of tools that you can use when your data are nominal scale. However, there are a lot of different tools that can be used for categorical data analysis, and this chapter only covers a few of the more common ones.</p>
<div id="goftest" class="section level2">
<h2><span class="header-section-number">12.1</span> The <span class="math inline">\(\chi^2\)</span> goodness-of-fit test</h2>
<p>The <span class="math inline">\(\chi^2\)</span> goodness-of-fit test is one of the oldest hypothesis tests around: it was invented by Karl Pearson around the turn of the century <span class="citation">(Pearson <a href="#ref-Pearson1900">1900</a>)</span>, with some corrections made later by Sir Ronald Fisher <span class="citation">(Fisher <a href="#ref-Fisher1922">1922</a><a href="#ref-Fisher1922">a</a>)</span>. To introduce the statistical problem that it addresses, let’s start with some psychology…</p>
<div id="the-cards-data" class="section level3">
<h3><span class="header-section-number">12.1.1</span> The cards data</h3>
<p>Over the years, there have been a lot of studies showing that humans have a lot of difficulties in simulating randomness. Try as we might to “act” random, we <em>think</em> in terms of patterns and structure, and so when asked to “do something at random”, what people actually do is anything but random. As a consequence, the study of human randomness (or non-randomness, as the case may be) opens up a lot of deep psychological questions about how we think about the world. With this in mind, let’s consider a very simple study. Suppose I asked people to imagine a shuffled deck of cards, and mentally pick one card from this imaginary deck “at random”. After they’ve chosen one card, I ask them to mentally select a second one. For both choices, what we’re going to look at is the suit (hearts, clubs, spades or diamonds) that people chose. After asking, say, <span class="math inline">\(N=200\)</span> people to do this, I’d like to look at the data and figure out whether or not the cards that people pretended to select were really random. The data are contained in the <code>randomness.Rdata</code> file, which contains a single data frame called <code>cards</code>. Let’s take a look:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>( lsr )
<span class="kw">load</span>( <span class="kw">file.path</span>(projecthome, <span class="st">&quot;data/randomness.Rdata&quot;</span> ))
<span class="kw">str</span>(cards)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    200 obs. of  3 variables:
##  $ id      : Factor w/ 200 levels &quot;subj1&quot;,&quot;subj10&quot;,..: 1 112 124 135 146 157 168 179 190 2 ...
##  $ choice_1: Factor w/ 4 levels &quot;clubs&quot;,&quot;diamonds&quot;,..: 4 2 3 4 3 1 3 2 4 2 ...
##  $ choice_2: Factor w/ 4 levels &quot;clubs&quot;,&quot;diamonds&quot;,..: 1 1 1 1 4 3 2 1 1 4 ...</code></pre>
<p>As you can see, the <code>cards</code> data frame contains three variables, an <code>id</code> variable that assigns a unique identifier to each participant, and the two variables <code>choice_1</code> and <code>choice_2</code> that indicate the card suits that people chose. Here’s the first few entries in the data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>( cards )</code></pre></div>
<pre><code>##      id choice_1 choice_2
## 1 subj1   spades    clubs
## 2 subj2 diamonds    clubs
## 3 subj3   hearts    clubs
## 4 subj4   spades    clubs
## 5 subj5   hearts   spades
## 6 subj6    clubs   hearts</code></pre>
<p>For the moment, let’s just focus on the first choice that people made. We’ll use the <code>table()</code> function to count the number of times that we observed people choosing each suit. I’ll save the table to a variable called <code>observed</code>, for reasons that will become clear very soon:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">observed &lt;-<span class="st"> </span><span class="kw">table</span>( cards<span class="op">$</span>choice_<span class="dv">1</span> )
observed</code></pre></div>
<pre><code>## 
##    clubs diamonds   hearts   spades 
##       35       51       64       50</code></pre>
<p>That little frequency table is quite helpful. Looking at it, there’s a bit of a hint that people <em>might</em> be more likely to select hearts than clubs, but it’s not completely obvious just from looking at it whether that’s really true, or if this is just due to chance. So we’ll probably have to do some kind of statistical analysis to find out, which is what I’m going to talk about in the next section.</p>
<p>Excellent. From this point on, we’ll treat this table as the data that we’re looking to analyse. However, since I’m going to have to talk about this data in mathematical terms (sorry!) it might be a good idea to be clear about what the notation is. In R, if I wanted to pull out the number of people that selected diamonds, I could do it by name by typing <code>observed[&quot;diamonds&quot;]</code> but, since <code>&quot;diamonds&quot;</code> is second element of the <code>observed</code> vector, it’s equally effective to refer to it as <code>observed[2]</code>. The mathematical notation for this is pretty similar, except that we shorten the human-readable word “observed” to the letter <span class="math inline">\(O\)</span>, and we use subscripts rather than brackets: so the second observation in our table is written as <code>observed[2]</code> in R, and is written as <span class="math inline">\(O_2\)</span> in maths. The relationship between the English descriptions, the R commands, and the mathematical symbols are illustrated below:</p>
<table>
<thead>
<tr class="header">
<th align="left">label</th>
<th align="center">index <span class="math inline">\(i\)</span></th>
<th align="center">math. symbol</th>
<th align="center">R command</th>
<th align="center">the value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">clubs <span class="math inline">\(\clubsuit\)</span></td>
<td align="center">1</td>
<td align="center"><span class="math inline">\(O_1\)</span></td>
<td align="center"><code>observed[1]</code></td>
<td align="center">35</td>
</tr>
<tr class="even">
<td align="left">diamonds <span class="math inline">\(\diamondsuit\)</span></td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(O_2\)</span></td>
<td align="center"><code>observed[2]</code></td>
<td align="center">51</td>
</tr>
<tr class="odd">
<td align="left">hearts <span class="math inline">\(\heartsuit\)</span></td>
<td align="center">3</td>
<td align="center"><span class="math inline">\(O_3\)</span></td>
<td align="center"><code>observed[3]</code></td>
<td align="center">64</td>
</tr>
<tr class="even">
<td align="left">spades <span class="math inline">\(\spadesuit\)</span></td>
<td align="center">4</td>
<td align="center"><span class="math inline">\(O_4\)</span></td>
<td align="center"><code>observed[4]</code></td>
<td align="center">50</td>
</tr>
</tbody>
</table>
<p>Hopefully that’s pretty clear. It’s also worth nothing that mathematicians prefer to talk about things in general rather than specific things, so you’ll also see the notation <span class="math inline">\(O_i\)</span>, which refers to the number of observations that fall within the <span class="math inline">\(i\)</span>-th category (where <span class="math inline">\(i\)</span> could be 1, 2, 3 or 4). Finally, if we want to refer to the set of all observed frequencies, statisticians group all of observed values into a vector, which I’ll refer to as <span class="math inline">\(O\)</span>. <span class="math display">\[
O = (O_1, O_2, O_3, O_4)
\]</span> Again, there’s nothing new or interesting here: it’s just notation. If I say that <span class="math inline">\(O~=~(35, 51, 64, 50)\)</span> all I’m doing is describing the table of observed frequencies (i.e., <code>observed</code>), but I’m referring to it using mathematical notation, rather than by referring to an R variable.</p>
</div>
<div id="the-null-hypothesis-and-the-alternative-hypothesis" class="section level3">
<h3><span class="header-section-number">12.1.2</span> The null hypothesis and the alternative hypothesis</h3>
<p>As the last section indicated, our research hypothesis is that “people don’t choose cards randomly”. What we’re going to want to do now is translate this into some statistical hypotheses, and construct a statistical test of those hypotheses. The test that I’m going to describe to you is <strong><em>Pearson’s <span class="math inline">\(\chi^2\)</span> goodness of fit test</em></strong>, and as is so often the case, we have to begin by carefully constructing our null hypothesis. In this case, it’s pretty easy. First, let’s state the null hypothesis in words:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(H_0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">All four suits are chosen with equal probability</td>
</tr>
</tbody>
</table>
<p>Now, because this is statistics, we have to be able to say the same thing in a mathematical way. To do this, let’s use the notation <span class="math inline">\(P_j\)</span> to refer to the true probability that the <span class="math inline">\(j\)</span>-th suit is chosen. If the null hypothesis is true, then each of the four suits has a 25% chance of being selected: in other words, our null hypothesis claims that <span class="math inline">\(P_1 = .25\)</span>, <span class="math inline">\(P_2 = .25\)</span>, <span class="math inline">\(P_3 = .25\)</span> and finally that <span class="math inline">\(P_4 = .25\)</span>. However, in the same way that we can group our observed frequencies into a vector <span class="math inline">\(O\)</span> that summarises the entire data set, we can use <span class="math inline">\(P\)</span> to refer to the probabilities that correspond to our null hypothesis. So if I let the vector <span class="math inline">\(P = (P_1, P_2, P_3, P_4)\)</span> refer to the collection of probabilities that describe our null hypothesis, then we have</p>
<p><span class="math display">\[
H_0: {P} = (.25, .25, .25, .25)
\]</span></p>
<p>In this particular instance, our null hypothesis corresponds to a vector of probabilities <span class="math inline">\(P\)</span> in which all of the probabilities are equal to one another. But this doesn’t have to be the case. For instance, if the experimental task was for people to imagine they were drawing from a deck that had twice as many clubs as any other suit, then the null hypothesis would correspond to something like <span class="math inline">\(P = (.4, .2, .2, .2)\)</span>. As long as the probabilities are all positive numbers, and they all sum to 1, them it’s a perfectly legitimate choice for the null hypothesis. However, the most common use of the goodness of fit test is to test a null hypothesis that all of the categories are equally likely, so we’ll stick to that for our example.</p>
<p>What about our alternative hypothesis, <span class="math inline">\(H_1\)</span>? All we’re really interested in is demonstrating that the probabilities involved aren’t all identical (that is, people’s choices weren’t completely random). As a consequence, the “human friendly” versions of our hypotheses look like this:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(H_0\)</span></th>
<th align="center"><span class="math inline">\(H_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">All four suits are chosen with equal probability</td>
<td align="center">At least one of the suit-choice probabilities <em>isn’t</em> .25</td>
</tr>
</tbody>
</table>
<p>and the “mathematician friendly” version is</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(H_0\)</span></th>
<th align="center"><span class="math inline">\(H_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P = (.25, .25, .25, .25)\)</span></td>
<td align="center"><span class="math inline">\(P \neq (.25,.25,.25,.25)\)</span></td>
</tr>
</tbody>
</table>
<p>Conveniently, the mathematical version of the hypotheses looks quite similar to an R command defining a vector. So maybe what I should do is store the <span class="math inline">\(P\)</span> vector in R as well, since we’re almost certainly going to need it later. And because I’m so imaginative, I’ll call this R vector <code>probabilities</code>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probabilities &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">clubs =</span> .<span class="dv">25</span>, <span class="dt">diamonds =</span> .<span class="dv">25</span>, <span class="dt">hearts =</span> .<span class="dv">25</span>, <span class="dt">spades =</span> .<span class="dv">25</span>) 
probabilities</code></pre></div>
<pre><code>##    clubs diamonds   hearts   spades 
##     0.25     0.25     0.25     0.25</code></pre>
</div>
<div id="the-goodness-of-fit-test-statistic" class="section level3">
<h3><span class="header-section-number">12.1.3</span> The “goodness of fit” test statistic</h3>
<p>At this point, we have our observed frequencies <span class="math inline">\(O\)</span> and a collection of probabilities <span class="math inline">\(P\)</span> corresponding the null hypothesis that we want to test. We’ve stored these in R as the corresponding variables <code>observed</code> and <code>probabilities</code>. What we now want to do is construct a test of the null hypothesis. As always, if we want to test <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span>, we’re going to need a test statistic. The basic trick that a goodness of fit test uses is to construct a test statistic that measures how “close” the data are to the null hypothesis. If the data don’t resemble what you’d “expect” to see if the null hypothesis were true, then it probably isn’t true. Okay, if the null hypothesis were true, what would we expect to see? Or, to use the correct terminology, what are the <strong><em>expected frequencies</em></strong>. There are <span class="math inline">\(N=200\)</span> observations, and (if the null is true) the probability of any one of them choosing a heart is <span class="math inline">\(P_3 = .25\)</span>, so I guess we’re expecting <span class="math inline">\(200 \times .25 = 50\)</span> hearts, right? Or, more specifically, if we let <span class="math inline">\(E_i\)</span> refer to “the number of category <span class="math inline">\(i\)</span> responses that we’re expecting if the null is true”, then <span class="math display">\[
E_i = N \times P_i
\]</span> This is pretty easy to calculate in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">200</span>  <span class="co"># sample size</span>
expected &lt;-<span class="st"> </span>N <span class="op">*</span><span class="st"> </span>probabilities <span class="co"># expected frequencies</span>
expected</code></pre></div>
<pre><code>##    clubs diamonds   hearts   spades 
##       50       50       50       50</code></pre>
<p>None of which is very surprising: if there are 200 observation that can fall into four categories, and we think that all four categories are equally likely, then on average we’d expect to see 50 observations in each category, right?</p>
<p>Now, how do we translate this into a test statistic? Clearly, what we want to do is compare the <em>expected</em> number of observations in each category (<span class="math inline">\(E_i\)</span>) with the <em>observed</em> number of observations in that category (<span class="math inline">\(O_i\)</span>). And on the basis of this comparison, we ought to be able to come up with a good test statistic. To start with, let’s calculate the difference between what the null hypothesis expected us to find and what we actually did find. That is, we calculate the “observed minus expected” difference score, <span class="math inline">\(O_i - E_i\)</span>. This is illustrated in the following table.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th align="right"><span class="math inline">\(\clubsuit\)</span></th>
<th align="right"><span class="math inline">\(\diamondsuit\)</span></th>
<th align="right"><span class="math inline">\(\heartsuit\)</span></th>
<th align="right"><span class="math inline">\(\spadesuit\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>expected frequency</td>
<td><span class="math inline">\(E_i\)</span></td>
<td align="right">50</td>
<td align="right">50</td>
<td align="right">50</td>
<td align="right">50</td>
</tr>
<tr class="even">
<td>observed frequency</td>
<td><span class="math inline">\(O_i\)</span></td>
<td align="right">35</td>
<td align="right">51</td>
<td align="right">64</td>
<td align="right">50</td>
</tr>
<tr class="odd">
<td>difference score</td>
<td><span class="math inline">\(O_i - E_i\)</span></td>
<td align="right">-15</td>
<td align="right">1</td>
<td align="right">14</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The same calculations can be done in R, using our <code>expected</code> and <code>observed</code> variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">observed <span class="op">-</span><span class="st"> </span>expected </code></pre></div>
<pre><code>## 
##    clubs diamonds   hearts   spades 
##      -15        1       14        0</code></pre>
<p>Regardless of whether we do the calculations by hand or whether we do them in R, it’s clear that people chose more hearts and fewer clubs than the null hypothesis predicted. However, a moment’s thought suggests that these raw differences aren’t quite what we’re looking for. Intuitively, it feels like it’s just as bad when the null hypothesis predicts too few observations (which is what happened with hearts) as it is when it predicts too many (which is what happened with clubs). So it’s a bit weird that we have a negative number for clubs and a positive number for heards. One easy way to fix this is to square everything, so that we now calculate the squared differences, <span class="math inline">\((E_i - O_i)^2\)</span>. As before, we could do this by hand, but it’s easier to do it in R…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(observed <span class="op">-</span><span class="st"> </span>expected)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## 
##    clubs diamonds   hearts   spades 
##      225        1      196        0</code></pre>
<p>Now we’re making progress. What we’ve got now is a collection of numbers that are big whenever the null hypothesis makes a bad prediction (clubs and hearts), but are small whenever it makes a good one (diamonds and spades). Next, for some technical reasons that I’ll explain in a moment, let’s also divide all these numbers by the expected frequency <span class="math inline">\(E_i\)</span>, so we’re actually calculating <span class="math inline">\(\frac{(E_i-O_i)^2}{E_i}\)</span>. Since <span class="math inline">\(E_i = 50\)</span> for all categories in our example, it’s not a very interesting calculation, but let’s do it anyway. The R command becomes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(observed <span class="op">-</span><span class="st"> </span>expected)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>expected</code></pre></div>
<pre><code>## 
##    clubs diamonds   hearts   spades 
##     4.50     0.02     3.92     0.00</code></pre>
<p>In effect, what we’ve got here are four different “error” scores, each one telling us how big a “mistake” the null hypothesis made when we tried to use it to predict our observed frequencies. So, in order to convert this into a useful test statistic, one thing we could do is just add these numbers up. The result is called the <strong><em>goodness of fit</em></strong> statistic, conventionally referred to either as <span class="math inline">\(X^2\)</span> or GOF. We can calculate it using this command in R</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>( (observed <span class="op">-</span><span class="st"> </span>expected)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>expected )</code></pre></div>
<pre><code>## [1] 8.44</code></pre>
<p>The formula for this statistic looks remarkably similar to the R command. If we let <span class="math inline">\(k\)</span> refer to the total number of categories (i.e., <span class="math inline">\(k=4\)</span> for our cards data), then the <span class="math inline">\(X^2\)</span> statistic is given by: <span class="math display">\[
X^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
\]</span> Intuitively, it’s clear that if <span class="math inline">\(X^2\)</span> is small, then the observed data <span class="math inline">\(O_i\)</span> are very close to what the null hypothesis predicted <span class="math inline">\(E_i\)</span>, so we’re going to need a large <span class="math inline">\(X^2\)</span> statistic in order to reject the null. As we’ve seen from our calculations, in our cards data set we’ve got a value of <span class="math inline">\(X^2 = 8.44\)</span>. So now the question becomes, is this a big enough value to reject the null?</p>
</div>
<div id="the-sampling-distribution-of-the-gof-statistic-advanced" class="section level3">
<h3><span class="header-section-number">12.1.4</span> The sampling distribution of the GOF statistic (advanced)</h3>
<p>To determine whether or not a particular value of <span class="math inline">\(X^2\)</span> is large enough to justify rejecting the null hypothesis, we’re going to need to figure out what the sampling distribution for <span class="math inline">\(X^2\)</span> would be if the null hypothesis were true. So that’s what I’m going to do in this section. I’ll show you in a fair amount of detail how this sampling distribution is constructed, and then – in the next section – use it to build up a hypothesis test. If you want to cut to the chase and are willing to take it on faith that the sampling distribution is a <strong><em>chi-squared (<span class="math inline">\(\chi^2\)</span>) distribution</em></strong> with <span class="math inline">\(k-1\)</span> degrees of freedom, you can skip the rest of this section. However, if you want to understand <em>why</em> the goodness of fit test works the way it does, read on…</p>
<p>Okay, let’s suppose that the null hypothesis is actually true. If so, then the true probability that an observation falls in the <span class="math inline">\(i\)</span>-th category is <span class="math inline">\(P_i\)</span> – after all, that’s pretty much the definition of our null hypothesis. Let’s think about what this actually means. If you think about it, this is kind of like saying that “nature” makes the decision about whether or not the observation ends up in category <span class="math inline">\(i\)</span> by flipping a weighted coin (i.e., one where the probability of getting a head is <span class="math inline">\(P_j\)</span>). And therefore, we can think of our observed frequency <span class="math inline">\(O_i\)</span> by imagining that nature flipped <span class="math inline">\(N\)</span> of these coins (one for each observation in the data set)… and exactly <span class="math inline">\(O_i\)</span> of them came up heads. Obviously, this is a pretty weird way to think about the experiment. But what it does (I hope) is remind you that we’ve actually seen this scenario before. It’s exactly the same set up that gave rise to the binomial distribution in Section <a href="probability.html#binomial">9.4</a>. In other words, if the null hypothesis is true, then it follows that our observed frequencies were generated by sampling from a binomial distribution: <span class="math display">\[
O_i \sim \mbox{Binomial}(P_i, N)
\]</span> Now, if you remember from our discussion of the central limit theorem (Section <a href="estimation.html#clt">10.3.3</a>), the binomial distribution starts to look pretty much identical to the normal distribution, especially when <span class="math inline">\(N\)</span> is large and when <span class="math inline">\(P_i\)</span> isn’t <em>too</em> close to 0 or 1. In other words as long as <span class="math inline">\(N \times P_i\)</span> is large enough – or, to put it another way, when the expected frequency <span class="math inline">\(E_i\)</span> is large enough – the theoretical distribution of <span class="math inline">\(O_i\)</span> is approximately normal. Better yet, if <span class="math inline">\(O_i\)</span> is normally distributed, then so is <span class="math inline">\((O_i - E_i)/\sqrt{E_i}\)</span> … since <span class="math inline">\(E_i\)</span> is a fixed value, subtracting off <span class="math inline">\(E_i\)</span> and dividing by <span class="math inline">\(\sqrt{E_i}\)</span> changes the mean and standard deviation of the normal distribution; but that’s all it does. Okay, so now let’s have a look at what our goodness of fit statistic actually <em>is</em>. What we’re doing is taking a bunch of things that are normally-distributed, squaring them, and adding them up. Wait. We’ve seen that before too! As we discussed in Section <a href="probability.html#otherdists">9.6</a>, when you take a bunch of things that have a standard normal distribution (i.e., mean 0 and standard deviation 1), square them, then add them up, then the resulting quantity has a chi-square distribution. So now we know that the null hypothesis predicts that the sampling distribution of the goodness of fit statistic is a chi-square distribution. Cool.</p>
<p>There’s one last detail to talk about, namely the degrees of freedom. If you remember back to Section <a href="probability.html#otherdists">9.6</a>, I said that if the number of things you’re adding up is <span class="math inline">\(k\)</span>, then the degrees of freedom for the resulting chi-square distribution is <span class="math inline">\(k\)</span>. Yet, what I said at the start of this section is that the actual degrees of freedom for the chi-square goodness of fit test is <span class="math inline">\(k-1\)</span>. What’s up with that? The answer here is that what we’re supposed to be looking at is the number of genuinely <em>independent</em> things that are getting added together. And, as I’ll go on to talk about in the next section, even though there’s <span class="math inline">\(k\)</span> things that we’re adding, only <span class="math inline">\(k-1\)</span> of them are truly independent; and so the degrees of freedom is actually only <span class="math inline">\(k-1\)</span>. That’s the topic of the next section.<a href="#fn171" class="footnoteRef" id="fnref171"><sup>171</sup></a></p>
</div>
<div id="degrees-of-freedom" class="section level3">
<h3><span class="header-section-number">12.1.5</span> Degrees of freedom</h3>
<div class="figure"><span id="fig:manychi"></span>
<img src="lsr_files/figure-html/manychi-1.png" alt="Chi-square distributions with different values for the &quot;degrees of freedom&quot;." width="672" />
<p class="caption">
Figure 12.1: Chi-square distributions with different values for the “degrees of freedom”.
</p>
</div>
<p>When I introduced the chi-square distribution in Section <a href="probability.html#otherdists">9.6</a>, I was a bit vague about what “<strong><em>degrees of freedom</em></strong>” actually <em>means</em>. Obviously, it matters: looking Figure <a href="chisquare.html#fig:manychi">12.1</a> you can see that if we change the degrees of freedom, then the chi-square distribution changes shape quite substantially. But what exactly <em>is</em> it? Again, when I introduced the distribution and explained its relationship to the normal distribution, I did offer an answer… it’s the number of “normally distributed variables” that I’m squaring and adding together. But, for most people, that’s kind of abstract, and not entirely helpful. What we really need to do is try to understand degrees of freedom in terms of our data. So here goes.</p>
<p>The basic idea behind degrees of freedom is quite simple: you calculate it by counting up the number of distinct “quantities” that are used to describe your data; and then subtracting off all of the “constraints” that those data must satisfy.<a href="#fn172" class="footnoteRef" id="fnref172"><sup>172</sup></a> This is a bit vague, so let’s use our cards data as a concrete example. We describe out data using four numbers, <span class="math inline">\(O_1\)</span>, <span class="math inline">\(O_2\)</span>, <span class="math inline">\(O_3\)</span> and <span class="math inline">\(O_4\)</span> corresponding to the observed frequencies of the four different categories (hearts, clubs, diamonds, spades). These four numbers are the <em>random outcomes</em> of our experiment. But, my experiment actually has a fixed constraint built into it: the sample size <span class="math inline">\(N\)</span>.<a href="#fn173" class="footnoteRef" id="fnref173"><sup>173</sup></a> That is, if we know how many people chose hearts, how many chose diamonds and how many chose clubs; then we’d be able to figure out exactly how many chose spades. In other words, although our data are described using four numbers, they only actually correspond to <span class="math inline">\(4-1 = 3\)</span> degrees of freedom. A slightly different way of thinking about it is to notice that there are four <em>probabilities</em> that we’re interested in (again, corresponding to the four different categories), but these probabilities must sum to one, which imposes a constraint. Therefore, the degrees of freedom is <span class="math inline">\(4-1 = 3\)</span>. Regardless of whether you want to think about it in terms of the observed frequencies or in terms of the probabilities, the answer is the same. In general, when running the chi-square goodness of fit test for an experiment involving <span class="math inline">\(k\)</span> groups, then the degrees of freedom will be <span class="math inline">\(k-1\)</span>.</p>
</div>
<div id="testing-the-null-hypothesis" class="section level3">
<h3><span class="header-section-number">12.1.6</span> Testing the null hypothesis</h3>
<div class="figure"><span id="fig:goftest"></span>
<img src="lsr_files/figure-html/goftest-1.png" alt="Illustration of how the hypothesis testing works for the chi-square goodness of fit test." width="672" />
<p class="caption">
Figure 12.2: Illustration of how the hypothesis testing works for the chi-square goodness of fit test.
</p>
</div>
<p>The final step in the process of constructing our hypothesis test is to figure out what the rejection region is. That is, what values of <span class="math inline">\(X^2\)</span> would lead is to reject the null hypothesis. As we saw earlier, large values of <span class="math inline">\(X^2\)</span> imply that the null hypothesis has done a poor job of predicting the data from our experiment, whereas small values of <span class="math inline">\(X^2\)</span> imply that it’s actually done pretty well. Therefore, a pretty sensible strategy would be to say there is some critical value, such that if <span class="math inline">\(X^2\)</span> is bigger than the critical value we reject the null; but if <span class="math inline">\(X^2\)</span> is smaller than this value we retain the null. In other words, to use the language we introduced in Chapter @ref(hypothesistesting the chi-squared goodness of fit test is always a <strong><em>one-sided test</em></strong>. Right, so all we have to do is figure out what this critical value is. And it’s pretty straightforward. If we want our test to have significance level of <span class="math inline">\(\alpha = .05\)</span> (that is, we are willing to tolerate a Type I error rate of 5%), then we have to choose our critical value so that there is only a 5% chance that <span class="math inline">\(X^2\)</span> could get to be that big if the null hypothesis is true. That is to say, we want the 95th percentile of the sampling distribution. This is illustrated in Figure <a href="chisquare.html#fig:goftest">12.2</a>.</p>
<p>Ah, but – I hear you ask – how do I calculate the 95th percentile of a chi-squared distribution with <span class="math inline">\(k-1\)</span> degrees of freedom? If only R had some function, called… oh, I don’t know, <code>qchisq()</code> … that would let you calculate this percentile (see Chapter <a href="probability.html#probability">9</a> if you’ve forgotten). Like this…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qchisq</span>( <span class="dt">p =</span> .<span class="dv">95</span>, <span class="dt">df =</span> <span class="dv">3</span> )</code></pre></div>
<pre><code>## [1] 7.814728</code></pre>
<p>So if our <span class="math inline">\(X^2\)</span> statistic is bigger than 7.81 or so, then we can reject the null hypothesis. Since we actually calculated that before (i.e., <span class="math inline">\(X^2 = 8.44\)</span>) we can reject the null. If we want an exact <span class="math inline">\(p\)</span>-value, we can calculate it using the <code>pchisq()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>( <span class="dt">q =</span> <span class="fl">8.44</span>, <span class="dt">df =</span> <span class="dv">3</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span> )</code></pre></div>
<pre><code>## [1] 0.03774185</code></pre>
<p>This is hopefully pretty straightforward, as long as you recall that the “<code>p</code>” form of the probability distribution functions in R always calculates the probability of getting a value of <em>less</em> than the value you entered (in this case 8.44). We want the opposite: the probability of getting a value of 8.44 or <em>more</em>. That’s why I told R to use the upper tail, not the lower tail. That said, it’s usually easier to calculate the <span class="math inline">\(p\)</span>-value this way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span><span class="op">-</span><span class="kw">pchisq</span>( <span class="dt">q =</span> <span class="fl">8.44</span>, <span class="dt">df =</span> <span class="dv">3</span> )</code></pre></div>
<pre><code>## [1] 0.03774185</code></pre>
<p>So, in this case we would reject the null hypothesis, since <span class="math inline">\(p &lt; .05\)</span>. And that’s it, basically. You now know “Pearson’s <span class="math inline">\(\chi^2\)</span> test for the goodness of fit”. Lucky you.</p>
</div>
<div id="gofTestInR" class="section level3">
<h3><span class="header-section-number">12.1.7</span> Doing the test in R</h3>
<p>Gosh darn it. Although we did manage to do everything in R as we were going through that little example, it does rather feel as if we’re typing too many things into the magic computing box. And I <em>hate</em> typing. Not surprisingly, R provides a function that will do all of these calculations for you. In fact, there are several different ways of doing it. The one that most people use is the <code>chisq.test()</code> function, which comes with every installation of R. I’ll show you how to use the <code>chisq.test()</code> function later on (in Section @ref(chisq.test), but to start out with I’m going to show you the <code>goodnessOfFitTest()</code> function in the <code>lsr</code> package, because it produces output that I think is easier for beginners to understand. It’s pretty straightforward: our raw data are stored in the variable <code>cards$choice_1</code>, right? If you want to test the null hypothesis that all four suits are equally likely, then (assuming you have the <code>lsr</code> package loaded) all you have to do is type this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">goodnessOfFitTest</span>( cards<span class="op">$</span>choice_<span class="dv">1</span> )</code></pre></div>
<pre><code>## 
##      Chi-square test against specified probabilities
## 
## Data variable:   cards$choice_1 
## 
## Hypotheses: 
##    null:        true probabilities are as specified
##    alternative: true probabilities differ from those specified
## 
## Descriptives: 
##          observed freq. expected freq. specified prob.
## clubs                35             50            0.25
## diamonds             51             50            0.25
## hearts               64             50            0.25
## spades               50             50            0.25
## 
## Test results: 
##    X-squared statistic:  8.44 
##    degrees of freedom:  3 
##    p-value:  0.038</code></pre>
<p>R then runs the test, and prints several lines of text. I’ll go through the output line by line, so that you can make sure that you understand what you’re looking at. The first two lines are just telling you things you already know:</p>
<pre><code>     Chi-square test against specified probabilities

Data variable:   cards$choice_1 </code></pre>
<p>The first line tells us what kind of hypothesis test we ran, and the second line tells us the name of the variable that we ran it on. After that comes a statement of what the null and alternative hypotheses are:</p>
<pre><code>Hypotheses: 
   null:        true probabilities are as specified
   alternative: true probabilities differ from those specified</code></pre>
<p>For a beginner, it’s kind of handy to have this as part of the output: it’s a nice reminder of what your null and alternative hypotheses are. Don’t get used to seeing this though. The vast majority of hypothesis tests in R aren’t so kind to novices. Most R functions are written on the assumption that you already understand the statistical tool that you’re using, so they don’t bother to include an explicit statement of the null and alternative hypothesis. The only reason that <code>goodnessOfFitTest()</code> actually does give you this is that I wrote it with novices in mind.</p>
<p>The next part of the output shows you the comparison between the observed frequencies and the expected frequencies:</p>
<pre><code>Descriptives: 
         observed freq. expected freq. specified prob.
clubs                35             50            0.25
diamonds             51             50            0.25
hearts               64             50            0.25
spades               50             50            0.25</code></pre>
<p>The first column shows what the observed frequencies were, the second column shows the expected frequencies according to the null hypothesis, and the third column shows you what the probabilities actually were according to the null. For novice users, I think this is helpful: you can look at this part of the output and check that it makes sense: if it doesn’t you might have typed something incorrecrtly.</p>
<p>The last part of the output is the “important” stuff: it’s the result of the hypothesis test itself. There are three key numbers that need to be reported: the value of the <span class="math inline">\(X^2\)</span> statistic, the degrees of freedom, and the <span class="math inline">\(p\)</span>-value:</p>
<pre><code>Test results: 
   X-squared statistic:  8.44 
   degrees of freedom:  3 
   p-value:  0.038 </code></pre>
<p>Notice that these are the same numbers that we came up with when doing the calculations the long way.</p>
</div>
<div id="specifying-a-different-null-hypothesis" class="section level3">
<h3><span class="header-section-number">12.1.8</span> Specifying a different null hypothesis</h3>
<p>At this point you might be wondering what to do if you want to run a goodness of fit test, but your null hypothesis is <em>not</em> that all categories are equally likely. For instance, let’s suppose that someone had made the theoretical prediction that people should choose red cards 60% of the time, and black cards 40% of the time (I’ve no idea why you’d predict that), but had no other preferences. If that were the case, the null hypothesis would be to expect 30% of the choices to be hearts, 30% to be diamonds, 20% to be spades and 20% to be clubs. This seems like a silly theory to me, and it’s pretty easy to test it using our data. All we need to do is specify the probabilities associated with the null hypothesis. We create a vector like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nullProbs &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">clubs =</span> .<span class="dv">2</span>, <span class="dt">diamonds =</span> .<span class="dv">3</span>, <span class="dt">hearts =</span> .<span class="dv">3</span>, <span class="dt">spades =</span> .<span class="dv">2</span>)
nullProbs</code></pre></div>
<pre><code>##    clubs diamonds   hearts   spades 
##      0.2      0.3      0.3      0.2</code></pre>
<p>Now that we have an explicitly specified null hypothesis, we include it in our command. This time round I’ll use the argument names properly. The data variable corresponds to the argument <code>x</code>, and the probabilities according to the null hypothesis correspond to the argument <code>p</code>. So our command is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">goodnessOfFitTest</span>( <span class="dt">x =</span> cards<span class="op">$</span>choice_<span class="dv">1</span>, <span class="dt">p =</span> nullProbs )</code></pre></div>
<pre><code>## 
##      Chi-square test against specified probabilities
## 
## Data variable:   cards$choice_1 
## 
## Hypotheses: 
##    null:        true probabilities are as specified
##    alternative: true probabilities differ from those specified
## 
## Descriptives: 
##          observed freq. expected freq. specified prob.
## clubs                35             40             0.2
## diamonds             51             60             0.3
## hearts               64             60             0.3
## spades               50             40             0.2
## 
## Test results: 
##    X-squared statistic:  4.742 
##    degrees of freedom:  3 
##    p-value:  0.192</code></pre>
<p>As you can see the null hypothesis and the expected frequencies are different to what they were last time. As a consequence our <span class="math inline">\(X^2\)</span> test statistic is different, and our <span class="math inline">\(p\)</span>-value is different too. Annoyingly, the <span class="math inline">\(p\)</span>-value is .192, so we can’t reject the null hypothesis. Sadly, despite the fact that the null hypothesis corresponds to a very silly theory, these data don’t provide enough evidence against it.</p>
</div>
<div id="chisqreport" class="section level3">
<h3><span class="header-section-number">12.1.9</span> How to report the results of the test</h3>
<p>So now you know how the test works, and you know how to do the test using a wonderful magic computing box. The next thing you need to know is how to write up the results. After all, there’s no point in designing and running an experiment and then analysing the data if you don’t tell anyone about it! So let’s now talk about what you need to do when reporting your analysis. Let’s stick with our card-suits example. If I wanted to write this result up for a paper or something, the conventional way to report this would be to write something like this:</p>
<blockquote>
<p>Of the 200 participants in the experiment, 64 selected hearts for their first choice, 51 selected diamonds, 50 selected spades, and 35 selected clubs. A chi-square goodness of fit test was conducted to test whether the choice probabilities were identical for all four suits. The results were significant (<span class="math inline">\(\chi^2(3) = 8.44, p&lt;.05\)</span>), suggesting that people did not select suits purely at random.</p>
</blockquote>
<p>This is pretty straightforward, and hopefully it seems pretty unremarkable. That said, there’s a few things that you should note about this description:</p>
<ul>
<li><em>The statistical test is preceded by the descriptive statistics</em>. That is, I told the reader something about what the data look like before going on to do the test. In general, this is good practice: always remember that your reader doesn’t know your data anywhere near as well as you do. So unless you describe it to them properly, the statistical tests won’t make any sense to them, and they’ll get frustrated and cry.</li>
<li><em>The description tells you what the null hypothesis being tested is</em>. To be honest, writers don’t always do this, but it’s often a good idea in those situations where some ambiguity exists; or when you can’t rely on your readership being intimately familiar with the statistical tools that you’re using. Quite often the reader might not know (or remember) all the details of the test that your using, so it’s a kind of politeness to “remind” them! As far as the goodness of fit test goes, you can usually rely on a scientific audience knowing how it works (since it’s covered in most intro stats classes). However, it’s still a good idea to be explicit about stating the null hypothesis (briefly!) because the null hypothesis can be different depending on what you’re using the test for. For instance, in the cards example my null hypothesis was that all the four suit probabilities were identical (i.e., <span class="math inline">\(P_1 = P_2 = P_3 = P_4 = 0.25\)</span>), but there’s nothing special about that hypothesis. I could just as easily have tested the null hypothesis that <span class="math inline">\(P_1 = 0.7\)</span> and <span class="math inline">\(P_2 = P_3 = P_4 = 0.1\)</span> using a goodness of fit test. So it’s helpful to the reader if you explain to them what your null hypothesis was. Also, notice that I described the null hypothesis in words, not in maths. That’s perfectly acceptable. You can describe it in maths if you like, but since most readers find words easier to read than symbols, most writers tend to describe the null using words if they can.</li>
<li><em>A “stat block” is included</em>. When reporting the results of the test itself, I didn’t just say that the result was significant, I included a “stat block” (i.e., the dense mathematical-looking part in the parentheses), which reports all the “raw” statistical data. For the chi-square goodness of fit test, the information that gets reported is the test statistic (that the goodness of fit statistic was 8.44), the information about the distribution used in the test (<span class="math inline">\(\chi^2\)</span> with 3 degrees of freedom, which is usually shortened to <span class="math inline">\(\chi^2(3)\)</span>), and then the information about whether the result was significant (in this case <span class="math inline">\(p&lt;.05\)</span>). The particular information that needs to go into the stat block is different for every test, and so each time I introduce a new test I’ll show you what the stat block should look like.<a href="#fn174" class="footnoteRef" id="fnref174"><sup>174</sup></a> However the general principle is that you should always provide enough information so that the reader could check the test results themselves if they really wanted to.</li>
<li><em>The results are interpreted</em>. In addition to indicating that the result was significant, I provided an interpretation of the result (i.e., that people didn’t choose randomly). This is also a kindness to the reader, because it tells them something about what they should believe about what’s going on in your data. If you don’t include something like this, it’s really hard for your reader to understand what’s going on.<a href="#fn175" class="footnoteRef" id="fnref175"><sup>175</sup></a></li>
</ul>
<p>As with everything else, your overriding concern should be that you <em>explain</em> things to your reader. Always remember that the point of reporting your results is to communicate to another human being. I cannot tell you just how many times I’ve seen the results section of a report or a thesis or even a scientific article that is just gibberish, because the writer has focused solely on making sure they’ve included all the numbers, and forgotten to actually communicate with the human reader.</p>
</div>
<div id="a-comment-on-statistical-notation-advanced" class="section level3">
<h3><span class="header-section-number">12.1.10</span> A comment on statistical notation (advanced)</h3>
<blockquote>
<p><em>Satan delights equally in statistics and in quoting scripture</em></p>
<p>– H.G. Wells</p>
</blockquote>
<p>If you’ve been reading very closely, and are as much of a mathematical pedant as I am, there is one thing about the way I wrote up the chi-square test in the last section that might be bugging you a little bit. There’s something that feels a bit wrong with writing “<span class="math inline">\(\chi^2(3) = 8.44\)</span>”, you might be thinking. After all, it’s the goodness of fit statistic that is equal to 8.44, so shouldn’t I have written <span class="math inline">\(X^2 = 8.44\)</span> or maybe GOF<span class="math inline">\(=8.44\)</span>? This seems to be conflating the <em>sampling distribution</em> (i.e., <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(df = 3\)</span>) with the <em>test statistic</em> (i.e., <span class="math inline">\(X^2\)</span>). Odds are you figured it was a typo, since <span class="math inline">\(\chi\)</span> and <span class="math inline">\(X\)</span> look pretty similar. Oddly, it’s not. Writing <span class="math inline">\(\chi^2(3) = 8.44\)</span> is essentially a highly condensed way of writing “the sampling distribution of the test statistic is <span class="math inline">\(\chi^2(3)\)</span>, and the value of the test statistic is 8.44”.</p>
<p>In one sense, this is kind of stupid. There are <em>lots</em> of different test statistics out there that turn out to have a chi-square sampling distribution: the <span class="math inline">\(X^2\)</span> statistic that we’ve used for our goodness of fit test is only one of many (albeit one of the most commonly encountered ones). In a sensible, perfectly organised world, we’d <em>always</em> have a separate name for the test statistic and the sampling distribution: that way, the stat block itself would tell you exactly what it was that the researcher had calculated. Sometimes this happens. For instance, the test statistic used in the Pearson goodness of fit test is written <span class="math inline">\(X^2\)</span>; but there’s a closely related test known as the <span class="math inline">\(G\)</span>-test<a href="#fn176" class="footnoteRef" id="fnref176"><sup>176</sup></a> , in which the test statistic is written as <span class="math inline">\(G\)</span>. As it happens, the Pearson goodness of fit test and the <span class="math inline">\(G\)</span>-test both test the same null hypothesis; and the sampling distribution is exactly the same (i.e., chi-square with <span class="math inline">\(k-1\)</span> degrees of freedom). If I’d done a <span class="math inline">\(G\)</span>-test for the cards data rather than a goodness of fit test, then I’d have ended up with a test statistic of <span class="math inline">\(G = 8.65\)</span>, which is slightly different from the <span class="math inline">\(X^2 = 8.44\)</span> value that I got earlier; and produces a slightly smaller <span class="math inline">\(p\)</span>-value of <span class="math inline">\(p = .034\)</span>. Suppose that the convention was to report the test statistic, then the sampling distribution, and then the <span class="math inline">\(p\)</span>-value. If that were true, then these two situations would produce different stat blocks: my original result would be written <span class="math inline">\(X^2 = 8.44, \chi^2(3), p = .038\)</span>, whereas the new version using the <span class="math inline">\(G\)</span>-test would be written as <span class="math inline">\(G = 8.65, \chi^2(3), p = .034\)</span>. However, using the condensed reporting standard, the original result is written <span class="math inline">\(\chi^2(3) = 8.44, p = .038\)</span>, and the new one is written <span class="math inline">\(\chi^2(3) = 8.65, p = .034\)</span>, and so it’s actually unclear which test I actually ran.</p>
<p>So why don’t we live in a world in which the contents of the stat block uniquely specifies what tests were ran? The deep reason is that life is messy. We (as users of statistical tools) want it to be nice and neat and organised… we want it to be <em>designed</em>, as if it were a product. But that’s not how life works: statistics is an intellectual discipline just as much as any other one, and as such it’s a massively distributed, partly-collaborative and partly-competitive project that no-one really understands completely. The things that you and I use as data analysis tools weren’t created by an Act of the Gods of Statistics; they were invented by lots of different people, published as papers in academic journals, implemented, corrected and modified by lots of other people, and then explained to students in textbooks by someone else. As a consequence, there’s a <em>lot</em> of test statistics that don’t even have names; and as a consequence they’re just given the same name as the corresponding sampling distribution. As we’ll see later, any test statistic that follows a <span class="math inline">\(\chi^2\)</span> distribution is commonly called a “chi-square statistic”; anything that follows a <span class="math inline">\(t\)</span>-distribution is called a “<span class="math inline">\(t\)</span>-statistic” and so on. But, as the <span class="math inline">\(X^2\)</span> versus <span class="math inline">\(G\)</span> example illustrates, two different things with the same sampling distribution are still, well, different.</p>
<p>As a consequence, it’s sometimes a good idea to be clear about what the actual test was that you ran, especially if you’re doing something unusual. If you just say “chi-square test”, it’s not actually clear what test you’re talking about. Although, since the two most common chi-square tests are the goodness of fit test and the independence test (Section <a href="chisquare.html#chisqindependence">12.2</a>), most readers with stats training can probably guess. Nevertheless, it’s something to be aware of.</p>
</div>
</div>
<div id="chisqindependence" class="section level2">
<h2><span class="header-section-number">12.2</span> The <span class="math inline">\(\chi^2\)</span> test of independence (or association)</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">GUARDBOT1:</td>
<td align="left">Halt!</td>
</tr>
<tr class="even">
<td align="left">GUARDBOT2:</td>
<td align="left">Be you robot or human?</td>
</tr>
<tr class="odd">
<td align="left">LEELA:</td>
<td align="left">Robot…we be.</td>
</tr>
<tr class="even">
<td align="left">FRY:</td>
<td align="left">Uh, yup! Just two robots out roboting it up! Eh?</td>
</tr>
<tr class="odd">
<td align="left">GUARDBOT1:</td>
<td align="left">Administer the test.</td>
</tr>
<tr class="even">
<td align="left">GUARDBOT2:</td>
<td align="left">Which of the following would you most prefer? A: A puppy, B: A pretty flower from your sweetie, or C: A large properly-formatted data file?</td>
</tr>
<tr class="odd">
<td align="left">GUARDBOT1:</td>
<td align="left">Choose!</td>
</tr>
</tbody>
</table>
<p>– Futurama, “Fear of a Bot Planet</p>
<p>The other day I was watching an animated documentary examining the quaint customs of the natives of the planet <em>Chapek 9</em>. Apparently, in order to gain access to their capital city, a visitor must prove that they’re a robot, not a human. In order to determine whether or not visitor is human, they ask whether the visitor prefers puppies, flowers or large, properly formatted data files. “Pretty clever,” I thought to myself “but what if humans and robots have the same preferences? That probably wouldn’t be a very good test then, would it?” As it happens, I got my hands on the testing data that the civil authorities of <em>Chapek 9</em> used to check this. It turns out that what they did was very simple… they found a bunch of robots and a bunch of humans and asked them what they preferred. I saved their data in a file called <code>chapek9.Rdata</code>, which I can now load and have a quick look at:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>( <span class="kw">file.path</span>(projecthome, <span class="st">&quot;data/chapek9.Rdata&quot;</span> ))
<span class="kw">str</span>(chapek9)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    180 obs. of  2 variables:
##  $ species: Factor w/ 2 levels &quot;robot&quot;,&quot;human&quot;: 1 2 2 2 1 2 2 1 2 1 ...
##  $ choice : Factor w/ 3 levels &quot;puppy&quot;,&quot;flower&quot;,..: 2 3 3 3 3 2 3 3 1 2 ...</code></pre>
<p>Okay, so we have a single data frame called <code>chapek9</code>, which contains two factors, <code>species</code> and <code>choice</code>. As always, it’s nice to have a quick look at the data,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(chapek9)</code></pre></div>
<pre><code>##   species choice
## 1   robot flower
## 2   human   data
## 3   human   data
## 4   human   data
## 5   robot   data
## 6   human flower</code></pre>
<p>and then take a <code>summary()</code>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(chapek9)</code></pre></div>
<pre><code>##   species      choice   
##  robot:87   puppy : 28  
##  human:93   flower: 43  
##             data  :109</code></pre>
<p>In total there are 180 entries in the data frame, one for each person (counting both robots and humans as “people”) who was asked to make a choice. Specifically, there’s 93 humans and 87 robots; and overwhelmingly the preferred choice is the data file. However, these summaries don’t address the question we’re interested in. To do that, we need a more detailed description of the data. What we want to do is look at the <code>choices</code> broken down <em>by</em> <code>species</code>. That is, we need to cross-tabulate the data (see Section <a href="datahandling.html#freqtables">7.1</a>). There’s quite a few ways to do this, as we’ve seen, but since our data are stored in a data frame, it’s convenient to use the <code>xtabs()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">chapekFrequencies &lt;-<span class="st"> </span><span class="kw">xtabs</span>( <span class="op">~</span><span class="st"> </span>choice <span class="op">+</span><span class="st"> </span>species, <span class="dt">data =</span> chapek9)
chapekFrequencies</code></pre></div>
<pre><code>##         species
## choice   robot human
##   puppy     13    15
##   flower    30    13
##   data      44    65</code></pre>
<p>That’s more or less what we’re after. So, if we add the row and column totals (which is convenient for the purposes of explaining the statistical tests), we would have a table like this,</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Robot</th>
<th align="center">Human</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Puppy</td>
<td align="center">13</td>
<td align="center">15</td>
<td align="center">28</td>
</tr>
<tr class="even">
<td>Flower</td>
<td align="center">30</td>
<td align="center">13</td>
<td align="center">43</td>
</tr>
<tr class="odd">
<td>Data file</td>
<td align="center">44</td>
<td align="center">65</td>
<td align="center">109</td>
</tr>
<tr class="even">
<td>Total</td>
<td align="center">87</td>
<td align="center">93</td>
<td align="center">180</td>
</tr>
<tr class="odd">
<td>which actual</td>
<td align="center">ly would</td>
<td align="center">be a nice</td>
<td align="center">way to report the descriptive statistics for this data set. In any case, it’s quite clear that the vast majority of the humans chose the data file, whereas the robots tended to be a lot more even in their preferences. Leaving aside the question of <em>why</em> the humans might be more likely to choose the data file for the moment (which does seem quite odd, admittedly), our first order of business is to determine if the discrepancy between human choices and robot choices in the data set is statistically significant.</td>
</tr>
</tbody>
</table>
<div id="constructing-our-hypothesis-test" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Constructing our hypothesis test</h3>
<p>How do we analyse this data? Specifically, since my <em>research</em> hypothesis is that “humans and robots answer the question in different ways”, how can I construct a test of the <em>null</em> hypothesis that “humans and robots answer the question the same way”? As before, we begin by establishing some notation to describe the data:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Robot</th>
<th align="left">Human</th>
<th align="left">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Puppy</td>
<td align="left"><span class="math inline">\(O_{11}\)</span></td>
<td align="left"><span class="math inline">\(O_{12}\)</span></td>
<td align="left"><span class="math inline">\(R_{1}\)</span></td>
</tr>
<tr class="even">
<td>Flower</td>
<td align="left"><span class="math inline">\(O_{21}\)</span></td>
<td align="left"><span class="math inline">\(O_{22}\)</span></td>
<td align="left"><span class="math inline">\(R_{2}\)</span></td>
</tr>
<tr class="odd">
<td>Data file</td>
<td align="left"><span class="math inline">\(O_{31}\)</span></td>
<td align="left"><span class="math inline">\(O_{32}\)</span></td>
<td align="left"><span class="math inline">\(R_{3}\)</span></td>
</tr>
<tr class="even">
<td>Total</td>
<td align="left"><span class="math inline">\(C_{1}\)</span></td>
<td align="left"><span class="math inline">\(C_{2}\)</span></td>
<td align="left"><span class="math inline">\(N\)</span></td>
</tr>
</tbody>
</table>
<p>In this notation we say that <span class="math inline">\(O_{ij}\)</span> is a count (observed frequency) of the number of respondents that are of species <span class="math inline">\(j\)</span> (robots or human) who gave answer <span class="math inline">\(i\)</span> (puppy, flower or data) when asked to make a choice. The total number of observations is written <span class="math inline">\(N\)</span>, as usual. Finally, I’ve used <span class="math inline">\(R_i\)</span> to denote the row totals (e.g., <span class="math inline">\(R_1\)</span> is the total number of people who chose the flower), and <span class="math inline">\(C_j\)</span> to denote the column totals (e.g., <span class="math inline">\(C_1\)</span> is the total number of robots).<a href="#fn177" class="footnoteRef" id="fnref177"><sup>177</sup></a></p>
<p>So now let’s think about what the null hypothesis says. If robots and humans are responding in the same way to the question, it means that the probability that “a robot says puppy” is the same as the probability that “a human says puppy”, and so on for the other two possibilities. So, if we use <span class="math inline">\(P_{ij}\)</span> to denote “the probability that a member of species <span class="math inline">\(j\)</span> gives response <span class="math inline">\(i\)</span>” then our null hypothesis is that:</p>
<table>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(H_0\)</span>:</td>
<td align="left">All of the following are true:</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span class="math inline">\(P_{11} = P_{12}\)</span> (same probability of saying puppy)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(P_{21} = P_{22}\)</span> (same probability of saying flower) and</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span class="math inline">\(P_{31} = P_{32}\)</span> (same probability of saying data).</td>
</tr>
</tbody>
</table>
<p>And actually, since the null hypothesis is claiming that the true choice probabilities don’t depend on the species of the person making the choice, we can let <span class="math inline">\(P_i\)</span> refer to this probability: e.g., <span class="math inline">\(P_1\)</span> is the true probability of choosing the puppy.</p>
<p>Next, in much the same way that we did with the goodness of fit test, what we need to do is calculate the expected frequencies. That is, for each of the observed counts <span class="math inline">\(O_{ij}\)</span>, we need to figure out what the null hypothesis would tell us to expect. Let’s denote this expected frequency by <span class="math inline">\(E_{ij}\)</span>. This time, it’s a little bit trickier. If there are a total of <span class="math inline">\(C_j\)</span> people that belong to species <span class="math inline">\(j\)</span>, and the true probability of anyone (regardless of species) choosing option <span class="math inline">\(i\)</span> is <span class="math inline">\(P_i\)</span>, then the expected frequency is just: <span class="math display">\[
E_{ij} = C_j \times P_i
\]</span> Now, this is all very well and good, but we have a problem. Unlike the situation we had with the goodness of fit test, the null hypothesis doesn’t actually specify a particular value for <span class="math inline">\(P_i\)</span>. It’s something we have to estimate (Chapter <a href="estimation.html#estimation">10</a>) from the data! Fortunately, this is pretty easy to do. If 28 out of 180 people selected the flowers, then a natural estimate for the probability of choosing flowers is <span class="math inline">\(28/180\)</span>, which is approximately <span class="math inline">\(.16\)</span>. If we phrase this in mathematical terms, what we’re saying is that our estimate for the probability of choosing option <span class="math inline">\(i\)</span> is just the row total divided by the total sample size: <span class="math display">\[
\hat{P}_i = \frac{R_i}{N}
\]</span> Therefore, our expected frequency can be written as the product (i.e. multiplication) of the row total and the column total, divided by the total number of observations:<a href="#fn178" class="footnoteRef" id="fnref178"><sup>178</sup></a> <span class="math display">\[
E_{ij} = \frac{R_i \times C_j}{N}
\]</span> Now that we’ve figured out how to calculate the expected frequencies, it’s straightforward to define a test statistic; following the exact same strategy that we used in the goodness of fit test. In fact, it’s pretty much the <em>same</em> statistic. For a contingency table with <span class="math inline">\(r\)</span> rows and <span class="math inline">\(c\)</span> columns, the equation that defines our <span class="math inline">\(X^2\)</span> statistic is <span class="math display">\[ 
X^2 = \sum_{i=1}^r  \sum_{j=1}^c \frac{({E}_{ij} - O_{ij})^2}{{E}_{ij}}
\]</span> The only difference is that I have to include two summation sign (i.e., <span class="math inline">\(\sum\)</span>) to indicate that we’re summing over both rows and columns. As before, large values of <span class="math inline">\(X^2\)</span> indicate that the null hypothesis provides a poor description of the data, whereas small values of <span class="math inline">\(X^2\)</span> suggest that it does a good job of accounting for the data. Therefore, just like last time, we want to reject the null hypothesis if <span class="math inline">\(X^2\)</span> is too large.</p>
<p>Not surprisingly, this statistic is <span class="math inline">\(\chi^2\)</span> distributed. All we need to do is figure out how many degrees of freedom are involved, which actually isn’t too hard. As I mentioned before, you can (usually) think of the degrees of freedom as being equal to the number of data points that you’re analysing, minus the number of constraints. A contingency table with <span class="math inline">\(r\)</span> rows and <span class="math inline">\(c\)</span> columns contains a total of <span class="math inline">\(r \times c\)</span> observed frequencies, so that’s the total number of observations. What about the constraints? Here, it’s slightly trickier. The answer is always the same <span class="math display">\[
df = (r-1)(c-1)
\]</span> but the explanation for <em>why</em> the degrees of freedom takes this value is different depending on the experimental design. For the sake of argument, let’s suppose that we had honestly intended to survey exactly 87 robots and 93 humans (column totals fixed by the experimenter), but left the row totals free to vary (row totals are random variables). Let’s think about the constraints that apply here. Well, since we deliberately fixed the column totals by Act of Experimenter, we have <span class="math inline">\(c\)</span> constraints right there. But, there’s actually more to it than that. Remember how our null hypothesis had some free parameters (i.e., we had to estimate the <span class="math inline">\(P_i\)</span> values)? Those matter too. I won’t explain why in this book, but every free parameter in the null hypothesis is rather like an additional constraint. So, how many of those are there? Well, since these probabilities have to sum to 1, there’s only <span class="math inline">\(r-1\)</span> of these. So our total degrees of freedom is: <span class="math display">\[
\begin{array}{rcl}
df &amp;=&amp; \mbox{(number of observations)} - \mbox{(number of constraints)} \\
&amp;=&amp; (rc) - (c + (r-1)) \\
&amp;=&amp; rc - c - r + 1 \\
&amp;=&amp; (r - 1)(c - 1)
\end{array}
\]</span> Alternatively, suppose that the only thing that the experimenter fixed was the total sample size <span class="math inline">\(N\)</span>. That is, we quizzed the first 180 people that we saw, and it just turned out that 87 were robots and 93 were humans. This time around our reasoning would be slightly different, but would still lead is to the same answer. Our null hypothesis still has <span class="math inline">\(r-1\)</span> free parameters corresponding to the choice probabilities, but it now <em>also</em> has <span class="math inline">\(c-1\)</span> free parameters corresponding to the species probabilities, because we’d also have to estimate the probability that a randomly sampled person turns out to be a robot.<a href="#fn179" class="footnoteRef" id="fnref179"><sup>179</sup></a> Finally, since we did actually fix the total number of observations <span class="math inline">\(N\)</span>, that’s one more constraint. So now we have, <span class="math inline">\(rc\)</span> observations, and <span class="math inline">\((c-1) + (r-1) + 1\)</span> constraints. What does that give? <span class="math display">\[
\begin{array}{rcl}
df &amp;=&amp; \mbox{(number of observations)} - \mbox{(number of constraints)} \\
&amp;=&amp; rc - ( (c-1) + (r-1) + 1) \\
&amp;=&amp; rc - c - r + 1 \\
&amp;=&amp; (r - 1)(c - 1)
\end{array}
\]</span> Amazing.</p>
</div>
<div id="AssocTestInR" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Doing the test in R</h3>
<p>Okay, now that we know how the test works, let’s have a look at how it’s done in R. As tempting as it is to lead you through the tedious calculations so that you’re forced to learn it the long way, I figure there’s no point. I already showed you how to do it the long way for the goodness of fit test in the last section, and since the test of independence isn’t conceptually any different, you won’t learn anything new by doing it the long way. So instead, I’ll go straight to showing you the easy way. As always, R lets you do it multiple ways. There’s the <code>chisq.test()</code> function, which I’ll talk about in Section @ref(chisq.test, but first I want to use the <code>associationTest()</code> function in the <code>lsr</code> package, which I think is easier on beginners. It works in the exact same way as the <code>xtabs()</code> function. Recall that, in order to produce the contingency table, we used this command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">xtabs</span>( <span class="dt">formula =</span> <span class="op">~</span>choice<span class="op">+</span>species, <span class="dt">data =</span> chapek9 )</code></pre></div>
<pre><code>##         species
## choice   robot human
##   puppy     13    15
##   flower    30    13
##   data      44    65</code></pre>
<p>The <code>associationTest()</code> function has exactly the same structure: it needs a <code>formula</code> that specifies which variables you’re cross-tabulating, and the name of a <code>data</code> frame that contains those variables. So the command is just this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">associationTest</span>( <span class="dt">formula =</span> <span class="op">~</span>choice<span class="op">+</span>species, <span class="dt">data =</span> chapek9 )</code></pre></div>
<pre><code>## 
##      Chi-square test of categorical association
## 
## Variables:   choice, species 
## 
## Hypotheses: 
##    null:        variables are independent of one another
##    alternative: some contingency exists between variables
## 
## Observed contingency table:
##         species
## choice   robot human
##   puppy     13    15
##   flower    30    13
##   data      44    65
## 
## Expected contingency table under the null hypothesis:
##         species
## choice   robot human
##   puppy   13.5  14.5
##   flower  20.8  22.2
##   data    52.7  56.3
## 
## Test results: 
##    X-squared statistic:  10.722 
##    degrees of freedom:  2 
##    p-value:  0.005 
## 
## Other information: 
##    estimated effect size (Cramer&#39;s v):  0.244</code></pre>
<p>Just like we did with the goodness of fit test, I’ll go through it line by line. The first two lines are, once again, just reminding you what kind of test you ran and what variables were used:</p>
<pre><code>     Chi-square test of categorical association

Variables:   choice, species </code></pre>
<p>Next, it tells you what the null and alternative hypotheses are (and again, I want to remind you not to get used to seeing these hypotheses written out so explicitly):</p>
<pre><code>Hypotheses: 
   null:        variables are independent of one another
   alternative: some contingency exists between variables</code></pre>
<p>Next, it shows you the observed contingency table that is being tested:</p>
<pre><code>Observed contingency table:
        species
choice   robot human
  puppy     13    15
  flower    30    13
  data      44    65</code></pre>
<p>and it also shows you what the expected frequencies would be if the null hypothesis were true:</p>
<pre><code>Expected contingency table under the null hypothesis:
        species
choice   robot human
  puppy   13.5  14.5
  flower  20.8  22.2
  data    52.7  56.3</code></pre>
<p>The next part describes the results of the hypothesis test itself:</p>
<pre><code>Test results: 
   X-squared statistic:  10.722 
   degrees of freedom:  2 
   p-value:  0.005 </code></pre>
<p>And finally, it reports a measure of effect size:</p>
<pre><code>Other information: 
   estimated effect size (Cramer&#39;s v):  0.244 </code></pre>
<p>You can ignore this bit for now. I’ll talk about it in just a moment.</p>
<p>This output gives us enough information to write up the result:</p>
<blockquote>
<p>Pearson’s <span class="math inline">\(\chi^2\)</span> revealed a significant association between species and choice (<span class="math inline">\(\chi^2(2) = 10.7, p &lt; .01\)</span>): robots appeared to be more likely to say that they prefer flowers, but the humans were more likely to say they prefer data.</p>
</blockquote>
<p>Notice that, once again, I provided a little bit of interpretation to help the human reader understand what’s going on with the data. Later on in my discussion section, I’d provide a bit more context. To illustrate the difference, here’s what I’d probably say later on:</p>
<blockquote>
<p>The fact that humans appeared to have a stronger preference for raw data files than robots is somewhat counterintuitive. However, in context it makes some sense: the civil authority on Chapek 9 has an unfortunate tendency to kill and dissect humans when they are identified. As such it seems most likely that the human participants did not respond honestly to the question, so as to avoid potentially undesirable consequences. This should be considered to be a substantial methodological weakness.</p>
</blockquote>
<p>This could be classified as a rather extreme example of a reactivity effect, I suppose. Obviously, in this case the problem is severe enough that the study is more or less worthless as a tool for understanding the difference preferences among humans and robots. However, I hope this illustrates the difference between getting a statistically significant result (our null hypothesis is rejected in favour of the alternative), and finding something of scientific value (the data tell us nothing of interest about our research hypothesis due to a big methodological flaw).</p>
</div>
<div id="postscript" class="section level3">
<h3><span class="header-section-number">12.2.3</span> Postscript</h3>
<p>I later found out the data were made up, and I’d been watching cartoons instead of doing work.</p>
</div>
</div>
<div id="yates" class="section level2">
<h2><span class="header-section-number">12.3</span> The continuity correction</h2>
<p>Okay, time for a little bit of a digression. I’ve been lying to you a little bit so far. There’s a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It’s called the “continuity correction”, or sometimes the <strong><em>Yates correction</em></strong>. Remember what I pointed out earlier: the <span class="math inline">\(\chi^2\)</span> test is based on an approximation, specifically on the assumption that binomial distribution starts to look like a normal distribution for large <span class="math inline">\(N\)</span>. One problem with this is that it often doesn’t quite work, especially when you’ve only got 1 degree of freedom (e.g., when you’re doing a test of independence on a <span class="math inline">\(2 \times 2\)</span> contingency table). The main reason for this is that the true sampling distribution for the <span class="math inline">\(X^2\)</span> statistic is actually discrete (because you’re dealing with categorical data!) but the <span class="math inline">\(\chi^2\)</span> distribution is continuous. This can introduce systematic problems. Specifically, when <span class="math inline">\(N\)</span> is small and when <span class="math inline">\(df=1\)</span>, the goodness of fit statistic tends to be “too big”, meaning that you actually have a bigger <span class="math inline">\(\alpha\)</span> value than you think (or, equivalently, the <span class="math inline">\(p\)</span> values are a bit too small). <span class="citation">Yates (<a href="#ref-Yates1934">1934</a>)</span> suggested a simple fix, in which you redefine the goodness of fit statistic as: <span class="math display">\[
X^2 = \sum_{i} \frac{(|E_i - O_i| - 0.5)^2}{E_i}
\]</span> Basically, he just subtracts off 0.5 everywhere. As far as I can tell from reading Yates’ paper, the correction is basically a hack. It’s not derived from any principled theory: rather, it’s based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. I feel obliged to explain this because you will sometimes see R (or any other software for that matter) introduce this correction, so it’s kind of useful to know what they’re about. You’ll know when it happens, because the R output will explicitly say that it has used a “continuity correction” or “Yates’ correction”.</p>
</div>
<div id="chisqeffectsize" class="section level2">
<h2><span class="header-section-number">12.4</span> Effect size</h2>
<p>As we discussed earlier (Section <a href="hypothesistesting.html#effectsize">11.8</a>), it’s becoming commonplace to ask researchers to report some measure of effect size. So, let’s suppose that you’ve run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness of fit test). Now you want to report a measure of effect size. That is, given that there is an association/deviation, how strong is it?</p>
<p>There are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won’t discuss all of them,<a href="#fn180" class="footnoteRef" id="fnref180"><sup>180</sup></a> but will instead focus on the most commonly reported measures of effect size.</p>
<p>By default, the two measures that people tend to report most frequently are the <span class="math inline">\(\phi\)</span> statistic and the somewhat superior version, known as Cram'er’s <span class="math inline">\(V\)</span>. Mathematically, they’re very simple. To calculate the <span class="math inline">\(\phi\)</span> statistic, you just divide your <span class="math inline">\(X^2\)</span> value by the sample size, and take the square root: <span class="math display">\[ 
\phi = \sqrt{\frac{X^2}{N}}
\]</span> The idea here is that the <span class="math inline">\(\phi\)</span> statistic is supposed to range between 0 (no at all association) and 1 (perfect association), but it doesn’t always do this when your contingency table is bigger than <span class="math inline">\(2 \times 2\)</span>, which is a total pain. For bigger tables it’s actually possible to obtain <span class="math inline">\(\phi&gt;1\)</span>, which is pretty unsatisfactory. So, to correct for this, people usually prefer to report the <span class="math inline">\(V\)</span> statistic proposed by <span class="citation">Cramér (<a href="#ref-Cramer1946">1946</a>)</span>. It’s a pretty simple adjustment to <span class="math inline">\(\phi\)</span>. If you’ve got a contingency table with <span class="math inline">\(r\)</span> rows and <span class="math inline">\(c\)</span> columns, then define <span class="math inline">\(k = \min(r,c)\)</span> to be the smaller of the two values. If so, then <strong><em>Cram'er’s <span class="math inline">\(V\)</span></em></strong> statistic is <span class="math display">\[
V = \sqrt{\frac{X^2}{N(k-1)}}
\]</span> And you’re done. This seems to be a fairly popular measure, presumably because it’s easy to calculate, and it gives answers that aren’t completely silly: you know that <span class="math inline">\(V\)</span> really does range from 0 (no at all association) to 1 (perfect association).</p>
<p>Calculating <span class="math inline">\(V\)</span> or <span class="math inline">\(\phi\)</span> is obviously pretty straightforward. So much so that the core packages in R don’t seem to have functions to do it, though other packages do. To save you the time and effort of finding one, I’ve included one in the <code>lsr</code> package, called <code>cramersV()</code>. It takes a contingency table as input, and prints out the measure of effect size:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cramersV</span>( chapekFrequencies )</code></pre></div>
<pre><code>## [1] 0.244058</code></pre>
<p>However, if you’re using the <code>associationTest()</code> function to do your analysis, then you won’t actually need to use this at all, because it reports the Cram'er’s <span class="math inline">\(V\)</span> statistic as part of the output.</p>
</div>
<div id="chisqassumptions" class="section level2">
<h2><span class="header-section-number">12.5</span> Assumptions of the test(s)</h2>
<p>All statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:</p>
<ul>
<li><em>Expected frequencies are sufficiently large</em>. Remember how in the previous section we saw that the <span class="math inline">\(\chi^2\)</span> sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in Chapter <a href="probability.html#probability">9</a> this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I’ve been able to discover , these seem to have been proposed as rough guidelines, not hard and fast rules; and they seem to be somewhat conservative [Larntz1978].</li>
<li><em>Data are independent of one another</em>. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here’s what I mean. Suppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards, and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I’d actually walked into the same ward 10 times, and in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 <em>observations</em> were massively non-independent… and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence “stuffs things up”. Sometimes it causes you to falsely reject the null, as the silly hospital example illustrats, but it can go the other way too. To give a slightly less stupid example, let’s consider what would happen if I’d done the cards experiment slightly differently: instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that <em>everyone</em> selects one heart, one club, one diamond and one spade (in keeping with the “representativeness heuristic”; Tversky &amp; Kahneman 1974). This is highly non-random behaviour from people, but in this case, I would get an observed frequency of 50 four all four suits. For this example, the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect… falsely retaining the null.</li>
</ul>
<p>If you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn.</p>
</div>
<div id="chisq.test" class="section level2">
<h2><span class="header-section-number">12.6</span> The most typical way to do chi-square tests in R</h2>
<p>When discussing how to do a chi-square goodness of fit test (Section <a href="chisquare.html#gofTestInR">12.1.7</a>) and the chi-square test of independence (Section <a href="chisquare.html#AssocTestInR">12.2.2</a>), I introduced you to two separate functions in the <code>lsr</code> package. We ran our goodness of fit tests using the <code>goodnessOfFitTest()</code> function, and our tests of independence (or association) using the <code>associationTest()</code> function. And both of those functions produced quite detailed output, showing you the relevant descriptive statistics, printing out explicit reminders of what the hypotheses are, and so on. When you’re first starting out, it can be very handy to be given this sort of guidance. However, once you start becoming a bit more proficient in statistics and in R it can start to get very tiresome. A real statistician hardly needs to be told what the null and alternative hypotheses for a chi-square test are, and if an advanced R user wants the descriptive statistics to be printed out, they know how to produce them!</p>
<p>For this reason, the basic <code>chisq.test()</code> function in R is a lot more terse in its output, and because the mathematics that underpin the goodness of fit test and the test of independence is basically the same in each case, it can run either test depending on what kind of input it is given. First, here’s the goodness of fit test. Suppose you have the frequency table <code>observed</code> that we used earlier,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">observed</code></pre></div>
<pre><code>## 
##    clubs diamonds   hearts   spades 
##       35       51       64       50</code></pre>
<p>If you want to run the goodness of fit test against the hypothesis that all four suits are equally likely to appear, then all you need to do is input this frequenct table to the <code>chisq.test()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( <span class="dt">x =</span> observed )</code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  observed
## X-squared = 8.44, df = 3, p-value = 0.03774</code></pre>
<p>Notice that the output is very compressed in comparison to the <code>goodnessOfFitTest()</code> function. It doesn’t bother to give you any descriptive statistics, it doesn’t tell you what null hypothesis is being tested, and so on. And as long as you already understand the test, that’s not a problem. Once you start getting familiar with R and with statistics, you’ll probably find that you prefer this simple output rather than the rather lengthy output that <code>goodnessOfFitTest()</code> produces. Anyway, if you want to change the null hypothesis, it’s exactly the same as before, just specify the probabilities using the <code>p</code> argument. For instance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( <span class="dt">x =</span> observed, <span class="dt">p =</span> <span class="kw">c</span>(.<span class="dv">2</span>, .<span class="dv">3</span>, .<span class="dv">3</span>, .<span class="dv">2</span>) )</code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  observed
## X-squared = 4.7417, df = 3, p-value = 0.1917</code></pre>
<p>Again, these are the same numbers that the <code>goodnessOfFitTest()</code> function reports at the end of the output. It just hasn’t included any of the other details.</p>
<p>What about a test of independence? As it turns out, the <code>chisq.test()</code> function is pretty clever.<a href="#fn181" class="footnoteRef" id="fnref181"><sup>181</sup></a> If you input a <em>cross-tabulation</em> rather than a simple frequency table, it realises that you’re asking for a test of independence and not a goodness of fit test. Recall that we already have this cross-tabulation stored as the <code>chapekFrequencies</code> variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">chapekFrequencies</code></pre></div>
<pre><code>##         species
## choice   robot human
##   puppy     13    15
##   flower    30    13
##   data      44    65</code></pre>
<p>To get the test of independence, all we have to do is feed this frequency table into the <code>chisq.test()</code> function like so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( chapekFrequencies )</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  chapekFrequencies
## X-squared = 10.722, df = 2, p-value = 0.004697</code></pre>
<p>Again, the numbers are the same as last time, it’s just that the output is very terse and doesn’t really explain what’s going on in the rather tedious way that <code>associationTest()</code> does. As before, my intuition is that when you’re just getting started it’s easier to use something like <code>associationTest()</code> because it shows you more detail about what’s going on, but later on you’ll probably find that <code>chisq.test()</code> is more convenient.</p>
</div>
<div id="fisherexacttest" class="section level2">
<h2><span class="header-section-number">12.7</span> The Fisher exact test</h2>
<p>What should you do if your cell counts are too small, but you’d still like to test the null hypothesis that the two variables are independent? One answer would be “collect more data”, but that’s far too glib: there are a lot of situations in which it would be either infeasible or unethical do that. If so, statisticians have a kind of moral obligation to provide scientists with better tests. In this instance, Fisher (1922) kindly provided the right answer to the question. To illustrate the basic idea, let’s suppose that we’re analysing data from a field experiment, looking at the emotional status of people who have been accused of witchcraft; some of whom are currently being burned at the stake.<a href="#fn182" class="footnoteRef" id="fnref182"><sup>182</sup></a> Unfortunately for the scientist (but rather fortunately for the general populace), it’s actually quite hard to find people in the process of being set on fire, so the cell counts are awfully small in some cases. The <code>salem.Rdata</code> file illustrates the point:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>( <span class="kw">file.path</span>(projecthome, <span class="st">&quot;data/salem.Rdata&quot;</span>))

salem.tabs &lt;-<span class="st"> </span><span class="kw">table</span>( trial )
<span class="kw">print</span>( salem.tabs )</code></pre></div>
<pre><code>##        on.fire
## happy   FALSE TRUE
##   FALSE     3    3
##   TRUE     10    0</code></pre>
<p>Looking at this data, you’d be hard pressed not to suspect that people not on fire are more likely to be happy than people on fire. However, the chi-square test makes this very hard to test because of the small sample size. If I try to do so, R gives me a warning message:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( salem.tabs )</code></pre></div>
<pre><code>## Warning in chisq.test(salem.tabs): Chi-squared approximation may be
## incorrect</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  salem.tabs
## X-squared = 3.3094, df = 1, p-value = 0.06888</code></pre>
<p>Speaking as someone who doesn’t want to be set on fire, I’d <em>really</em> like to be able to get a better answer than this. This is where <strong><em>Fisher’s exact test</em></strong>  comes in very handy.</p>
<p>The Fisher exact test works somewhat differently to the chi-square test (or in fact any of the other hypothesis tests that I talk about in this book) insofar as it doesn’t have a test statistic; it calculates the <span class="math inline">\(p\)</span>-value “directly”. I’ll explain the basics of how the test works for a <span class="math inline">\(2 \times 2\)</span> contingency table, though the test works fine for larger tables. As before, let’s have some notation:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Happy</th>
<th align="left">Sad</th>
<th align="left">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Set on fire</td>
<td align="left"><span class="math inline">\(O_{11}\)</span></td>
<td align="left"><span class="math inline">\(O_{12}\)</span></td>
<td align="left"><span class="math inline">\(R_{1}\)</span></td>
</tr>
<tr class="even">
<td>Not set on fire</td>
<td align="left"><span class="math inline">\(O_{21}\)</span></td>
<td align="left"><span class="math inline">\(O_{22}\)</span></td>
<td align="left"><span class="math inline">\(R_{2}\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="left"><span class="math inline">\(C_{1}\)</span></td>
<td align="left"><span class="math inline">\(C_{2}\)</span></td>
<td align="left"><span class="math inline">\(N\)</span></td>
</tr>
</tbody>
</table>
<p>In order to construct the test Fisher treats both the row and column totals (<span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span>, <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>) are known, fixed quantities; and then calculates the probability that we would have obtained the observed frequencies that we did (<span class="math inline">\(O_{11}\)</span>, <span class="math inline">\(O_{12}\)</span>, <span class="math inline">\(O_{21}\)</span> and <span class="math inline">\(O_{22}\)</span>) given those totals. In the notation that we developed in Chapter <a href="probability.html#probability">9</a> this is written: <span class="math display">\[
P(O_{11}, O_{12}, O_{21}, O_{22} \ | \ R_1, R_2, C_1, C_2) 
\]</span> and as you might imagine, it’s a slightly tricky exercise to figure out what this probability is, but it turns out that this probability is described by a distribution known as the <em>hypergeometric distribution</em>.<a href="#fn183" class="footnoteRef" id="fnref183"><sup>183</sup></a> Now that we know this, what we have to do to calculate our <span class="math inline">\(p\)</span>-value is calculate the probability of observing this particular table <em>or a table that is “more extreme”</em>.<a href="#fn184" class="footnoteRef" id="fnref184"><sup>184</sup></a> Back in the 1920s, computing this sum was daunting even in the simplest of situations, but these days it’s pretty easy as long as the tables aren’t too big and the sample size isn’t too large. The conceptually tricky issue is to figure out what it means to say that one contingency table is more “extreme” than another. The easiest solution is to say that the table with the lowest probability is the most extreme. This then gives us the <span class="math inline">\(p\)</span>-value.</p>
<p>The implementation of the test in R is via the <code>fisher.test()</code> function. Here’s how it is used:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fisher.test</span>( salem.tabs )</code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  salem.tabs
## p-value = 0.03571
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.000000 1.202913
## sample estimates:
## odds ratio 
##          0</code></pre>
<p>This is a bit more output than we got from some of our earlier tests. The main thing we’re interested in here is the <span class="math inline">\(p\)</span>-value, which in this case is small enough (<span class="math inline">\(p=.036\)</span>) to justify rejecting the null hypothesis that people on fire are just as happy as people not on fire.</p>
</div>
<div id="mcnemar" class="section level2">
<h2><span class="header-section-number">12.8</span> The McNemar test</h2>
<p>Suppose you’ve been hired to work for the <em>Australian Generic Political Party</em> (AGPP), and part of your job is to find out how effective the AGPP political advertisements are. So, what you do, is you put together a sample of <span class="math inline">\(N=100\)</span> people, and ask them to watch the AGPP ads. Before they see anything, you ask them if they intend to vote for the AGPP; and then after showing the ads, you ask them again, to see if anyone has changed their minds. Obviously, if you’re any good at your job, you’d also do a whole lot of other things too, but let’s consider just this one simple experiment. One way to describe your data is via the following contingency table:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Before</th>
<th align="center">After</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Yes</td>
<td align="center">30</td>
<td align="center">10</td>
<td align="center">40</td>
</tr>
<tr class="even">
<td>No</td>
<td align="center">70</td>
<td align="center">90</td>
<td align="center">160</td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center">100</td>
<td align="center">100</td>
<td align="center">200</td>
</tr>
</tbody>
</table>
<p>At first pass, you might think that this situation lends itself to the Pearson <span class="math inline">\(\chi^2\)</span> test of independence (as per Section <a href="chisquare.html#chisqindependence">12.2</a>). However, a little bit of thought reveals that we’ve got a problem: we have 100 participants, but 200 observations. This is because each person has provided us with an answer in <em>both</em> the before column and the after column. What this means is that the 200 observations aren’t independent of each other: if voter A says “yes” the first time and voter B says “no”, then you’d expect that voter A is more likely to say “yes” the second time than voter B! The consequence of this is that the usual <span class="math inline">\(\chi^2\)</span> test won’t give trustworthy answers due to the violation of the independence assumption. Now, if this were a really uncommon situation, I wouldn’t be bothering to waste your time talking about it. But it’s not uncommon at all: this is a <em>standard</em> repeated measures design, and none of the tests we’ve considered so far can handle it. Eek.</p>
<p>The solution to the problem was published by <span class="citation">McNemar (<a href="#ref-McNemar1947">1947</a>)</span>. The trick is to start by tabulating your data in a slightly different way:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Before: Yes</th>
<th align="center">Before: No</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>After: Yes</td>
<td align="center">5</td>
<td align="center">5</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td>After: No</td>
<td align="center">25</td>
<td align="center">65</td>
<td align="center">90</td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center">30</td>
<td align="center">70</td>
<td align="center">100</td>
</tr>
</tbody>
</table>
<p>This is exactly the same data, but it’s been rewritten so that each of our 100 participants appears in only one cell. Because we’ve written our data this way, the independence assumption is now satisfied, and this is a contingency table that we <em>can</em> use to construct an <span class="math inline">\(X^2\)</span> goodness of fit statistic. However, as we’ll see, we need to do it in a slightly nonstandard way. To see what’s going on, it helps to label the entries in our table a little differently:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Before: Yes</th>
<th align="center">Before: No</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>After: Yes</td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
<td align="center"><span class="math inline">\(a+b\)</span></td>
</tr>
<tr class="even">
<td>After: No</td>
<td align="center"><span class="math inline">\(c\)</span></td>
<td align="center"><span class="math inline">\(d\)</span></td>
<td align="center"><span class="math inline">\(c+d\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"><span class="math inline">\(a+c\)</span></td>
<td align="center"><span class="math inline">\(b+d\)</span></td>
<td align="center"><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>Next, let’s think about what our null hypothesis is: it’s that the “before” test and the “after” test have the same proportion of people saying “Yes, I will vote for AGPP”. Because of the way that we have rewritten the data, it means that we’re now testing the hypothesis that the <em>row totals</em> and <em>column totals</em> come from the same distribution. Thus, the null hypothesis in McNemar’s test is that we have “marginal homogeneity”. That is, the row totals and column totals have the same distribution: <span class="math inline">\(P_a + P_b = P_a + P_c\)</span>, and similarly that <span class="math inline">\(P_c + P_d = P_b + P_d\)</span>. Notice that this means that the null hypothesis actually simplifies to <span class="math inline">\(P_b = P_c\)</span>. In other words, as far as the McNemar test is concerned, it’s only the off-diagonal entries in this table (i.e., <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span>) that matter! After noticing this, the <strong><em>McNemar test of marginal homogeneity</em></strong> is no different to a usual <span class="math inline">\(\chi^2\)</span> test. After applying the Yates correction, our test statistic becomes: <span class="math display">\[
X^2 = \frac{(|b-c| - 0.5)^2}{b+c}
\]</span> or, to revert to the notation that we used earlier in this chapter: <span class="math display">\[
X^2 = \frac{(|O_{12}-O_{21}| - 0.5)^2}{O_{12} + O_{21}}
\]</span> and this statistic has an (approximately) <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(df=1\)</span>. However, remember that – just like the other <span class="math inline">\(\chi^2\)</span> tests – it’s only an approximation, so you need to have reasonably large expected cell counts for it to work.</p>
<div id="doing-the-mcnemar-test-in-r" class="section level3">
<h3><span class="header-section-number">12.8.1</span> Doing the McNemar test in R</h3>
<p>Now that you know what the McNemar test is all about, lets actually run one. The <code>agpp.Rdata</code> file contains the raw data that I discussed previously, so let’s have a look at it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">file.path</span>(projecthome, <span class="st">&quot;data/agpp.Rdata&quot;</span>))
<span class="kw">str</span>(agpp)     </code></pre></div>
<pre><code>## &#39;data.frame&#39;:    100 obs. of  3 variables:
##  $ id             : Factor w/ 100 levels &quot;subj.1&quot;,&quot;subj.10&quot;,..: 1 13 24 35 46 57 68 79 90 2 ...
##  $ response_before: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 2 1 1 1 1 1 1 ...
##  $ response_after : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 1 1 2 1 1 ...</code></pre>
<p>The <code>agpp</code> data frame contains three variables, an <code>id</code> variable that labels each participant in the data set (we’ll see why that’s useful in a moment), a <code>response_before</code> variable that records the person’s answer when they were asked the question the first time, and a <code>response_after</code> variable that shows the answer that they gave when asked the same question a second time. As usual, here’s the first 6 entries:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(agpp)</code></pre></div>
<pre><code>##       id response_before response_after
## 1 subj.1              no            yes
## 2 subj.2             yes             no
## 3 subj.3             yes             no
## 4 subj.4             yes             no
## 5 subj.5              no             no
## 6 subj.6              no             no</code></pre>
<p>and here’s a summary:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(agpp)     </code></pre></div>
<pre><code>##         id     response_before response_after
##  subj.1  : 1   no :70          no :90        
##  subj.10 : 1   yes:30          yes:10        
##  subj.100: 1                                 
##  subj.11 : 1                                 
##  subj.12 : 1                                 
##  subj.13 : 1                                 
##  (Other) :94</code></pre>
<p>Notice that each participant appears only once in this data frame. When we tabulate this data frame using <code>xtabs()</code>, we get the appropriate table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">right.table &lt;-<span class="st"> </span><span class="kw">xtabs</span>( <span class="op">~</span><span class="st"> </span>response_before <span class="op">+</span><span class="st"> </span>response_after, <span class="dt">data =</span> agpp)
<span class="kw">print</span>( right.table )</code></pre></div>
<pre><code>##                response_after
## response_before no yes
##             no  65   5
##             yes 25   5</code></pre>
<p>and from there, we can run the McNemar test by using the <code>mcnemar.test()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcnemar.test</span>( right.table )</code></pre></div>
<pre><code>## 
##  McNemar&#39;s Chi-squared test with continuity correction
## 
## data:  right.table
## McNemar&#39;s chi-squared = 12.033, df = 1, p-value = 0.0005226</code></pre>
<p>And we’re done. We’ve just run a McNemar’s test to determine if people were just as likely to vote AGPP after the ads as they were before hand. The test was significant (<span class="math inline">\(\chi^2(1) = 12.04, p&lt;.001\)</span>), suggesting that they were not. And in fact, it looks like the ads had a negative effect: people were less likely to vote AGPP after seeing the ads. Which makes a lot of sense when you consider the quality of a typical political advertisement.</p>
</div>
</div>
<div id="whats-the-difference-between-mcnemar-and-independence" class="section level2">
<h2><span class="header-section-number">12.9</span> What’s the difference between McNemar and independence?</h2>
<p>Let’s go all the way back to the beginning of the chapter, and look at the <code>cards</code> data set again. If you recall, the actual experimental design that I described involved people making <em>two</em> choices. Because we have information about the first choice and the second choice that everyone made, we can construct the following contingency table that cross-tabulates the first choice against the second choice.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cardChoices &lt;-<span class="st"> </span><span class="kw">xtabs</span>( <span class="op">~</span><span class="st"> </span>choice_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>choice_<span class="dv">2</span>, <span class="dt">data =</span> cards )
cardChoices</code></pre></div>
<pre><code>##           choice_2
## choice_1   clubs diamonds hearts spades
##   clubs       10        9     10      6
##   diamonds    20        4     13     14
##   hearts      20       18      3     23
##   spades      18       13     15      4</code></pre>
<p>Suppose I wanted to know whether the choice you make the second time is dependent on the choice you made the first time. This is where a test of independence is useful, and what we’re trying to do is see if there’s some relationship between the rows and columns of this table. Here’s the result:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( cardChoices )</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  cardChoices
## X-squared = 29.237, df = 9, p-value = 0.0005909</code></pre>
<p>Alternatively, suppose I wanted to know if <em>on average</em>, the frequencies of suit choices were different the second time than the first time. In that situation, what I’m really trying to see if the row totals in <code>cardChoices</code> (i.e., the frequencies for <code>choice_1</code>) are different from the column totals (i.e., the frequencies for <code>choice_2</code>). That’s when you use the McNemar test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcnemar.test</span>( cardChoices )</code></pre></div>
<pre><code>## 
##  McNemar&#39;s Chi-squared test
## 
## data:  cardChoices
## McNemar&#39;s chi-squared = 16.033, df = 6, p-value = 0.01358</code></pre>
<p>Notice that the results are different! These aren’t the same test.</p>
</div>
<div id="summary-10" class="section level2">
<h2><span class="header-section-number">12.10</span> Summary</h2>
<p>The key ideas discussed in this chapter are:</p>
<ul>
<li>The chi-square goodness of fit test (Section <a href="chisquare.html#goftest">12.1</a>) is used when you have a table of observed frequencies of different categories; and the null hypothesis gives you a set of “known” probabilities to compare them to. You can either use the <code>goodnessOfFitTest()</code> function in the <code>lsr</code> package to run this test, or the <code>chisq.test()</code> function.</li>
<li>The chi-square test of independence (Section <a href="chisquare.html#chisqindependence">12.2</a>) is used when you have a contingency table (cross-tabulation) of two categorical variables. The null hypothesis is that there is no relationship/association between the variables. You can either use the <code>associationTest()</code> function in the <code>lsr</code> package, or you can use <code>chisq.test()</code>.</li>
<li>Effect size for a contingency table can be measured in several ways (Section <a href="chisquare.html#chisqeffectsize">12.4</a>). In particular we noted the Cramer’s <span class="math inline">\(V\)</span> statistic, which can be calculated using <code>cramersV()</code>. This is also part of the output produced by <code>associationTest()</code>.</li>
<li>Both versions of the Pearson test rely on two assumptions: that the expected frequencies are sufficiently large, and that the observations are independent (Section <a href="chisquare.html#chisqassumptions">12.5</a>). The Fisher exact test (Section <a href="chisquare.html#fisherexacttest">12.7</a>) can be used when the expected frequencies are small, <code>fisher.test(x = contingency.table)</code>. The McNemar test (Section <a href="chisquare.html#mcnemar">12.8</a>) can be used for some kinds of violations of independence, <code>mcnemar.test(x = contingency.table)</code>.</li>
</ul>
<p>If you’re interested in learning more about categorical data analysis, a good first choice would be <span class="citation">Agresti (<a href="#ref-Agresti1996">1996</a>)</span> which, as the title suggests, provides an <em>Introduction to Categorical Data Analysis</em>. If the introductory book isn’t enough for you (or can’t solve the problem you’re working on) you could consider <span class="citation">Agresti (<a href="#ref-Agresti2002">2002</a>)</span>, <em>Categorical Data Analysis</em>. The latter is a more advanced text, so it’s probably not wise to jump straight from this book to that one.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Pearson1900">
<p>Pearson, K. 1900. “On the Criterion That a Given System of Deviations from the Probable in the Case of a Correlated System of Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from Random Sampling.” <em>Philosophical Magazine</em> 50: 157–75.</p>
</div>
<div id="ref-Fisher1922">
<p>Fisher, R. A. 1922a. “On the Interpretation of <span class="math inline">\(\chi^2\)</span> from Contingency Tables, and the Calculation of <span class="math inline">\(p\)</span>.” <em>Journal of the Royal Statistical Society</em> 84: 87–94.</p>
</div>
<div id="ref-Yates1934">
<p>Yates, F. 1934. “Contingency Tables Involving Small Numbers and the <span class="math inline">\(\chi^2\)</span> Test.” <em>Supplement to the Journal of the Royal Statistical Society</em> 1: 217–35.</p>
</div>
<div id="ref-Cramer1946">
<p>Cramér, H. 1946. <em>Mathematical Methods of Statistics</em>. Princeton: Princeton University Press.</p>
</div>
<div id="ref-McNemar1947">
<p>McNemar, Q. 1947. “Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages.” <em>Psychometrika</em> 12: 153–57.</p>
</div>
<div id="ref-Agresti1996">
<p>Agresti, A. 1996. <em>An Introduction to Categorical Data Analysis</em>. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-Agresti2002">
<p>Agresti, A. 2002. <em>Categorical Data Analysis</em>. 2nd ed. Hoboken, NJ: Wiley.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="171">
<li id="fn171"><p>I should point out that this issue does complicate the story somewhat: I’m not going to cover it in this book, but there’s a sneaky trick that you can do to rewrite the equation for the goodness of fit statistic as a sum over <span class="math inline">\(k-1\)</span> independent things. When we do so we get the “proper” sampling distribution, which is chi-square with <span class="math inline">\(k-1\)</span> degrees of freedom. In fact, in order to get the maths to work out properly, you actually have to rewrite things that way. But it’s beyond the scope of an introductory book to show the maths in that much detail: all I wanted to do is give you a sense of why the goodness of fit statistic is associated with the chi-squared distribution.<a href="chisquare.html#fnref171">↩</a></p></li>
<li id="fn172"><p>I feel obliged to point out that this is an over-simplification. It works nicely for quite a few situations; but every now and then we’ll come across degrees of freedom values that aren’t whole numbers. Don’t let this worry you too much – when you come across this, just remind yourself that “degrees of freedom” is actually a bit of a messy concept, and that the nice simple story that I’m telling you here isn’t the whole story. For an introductory class, it’s usually best to stick to the simple story: but I figure it’s best to warn you to expect this simple story to fall apart. If I didn’t give you this warning, you might start getting confused when you see <span class="math inline">\(df = 3.4\)</span> or something; and (incorrectly) thinking that you had misunderstood something that I’ve taught you, rather than (correctly) realising that there’s something that I haven’t told you.<a href="chisquare.html#fnref172">↩</a></p></li>
<li id="fn173"><p>In practice, the sample size isn’t always fixed… e.g., we might run the experiment over a fixed period of time, and the number of people participating depends on how many people show up. That doesn’t matter for the current purposes.<a href="chisquare.html#fnref173">↩</a></p></li>
<li id="fn174"><p>Well, sort of. The conventions for how statistics should be reported tend to differ somewhat from discipline to discipline; I’ve tended to stick with how things are done in psychology, since that’s what I do. But the general principle of providing enough information to the reader to allow them to check your results is pretty universal, I think.<a href="chisquare.html#fnref174">↩</a></p></li>
<li id="fn175"><p>To some people, this advice might sound odd, or at least in conflict with the “usual” advice on how to write a technical report. Very typically, students are told that the “results” section of a report is for describing the data and reporting statistical analysis; and the “discussion” section is for providing interpretation. That’s true as far as it goes, but I think people often interpret it way too literally. The way I usually approach it is to provide a quick and simple interpretation of the data in the results section, so that my reader understands what the data are telling us. Then, in the discussion, I try to tell a bigger story; about how my results fit with the rest of the scientific literature. In short; don’t let the “interpretation goes in the discussion” advice turn your results section into incomprehensible garbage. Being understood by your reader is <em>much</em> more important.<a href="chisquare.html#fnref175">↩</a></p></li>
<li id="fn176"><p>Complicating matters, the <span class="math inline">\(G\)</span>-test is a special case of a whole class of tests that are known as <em>likelihood ratio tests</em>. I don’t cover LRTs in this book, but they are quite handy things to know about.<a href="chisquare.html#fnref176">↩</a></p></li>
<li id="fn177"><p>A technical note. The way I’ve described the test pretends that the column totals are fixed (i.e., the researcher intended to survey 87 robots and 93 humans) and the row totals are random (i.e., it just turned out that 28 people chose the puppy). To use the terminology from my mathematical statistics textbook <span class="citation">(Hogg, McKean, and Craig <a href="#ref-Hogg2005">2005</a>)</span> I should technically refer to this situation as a chi-square test of homogeneity; and reserve the term chi-square test of independence for the situation where both the row and column totals are random outcomes of the experiment. In the initial drafts of this book that’s exactly what I did. However, it turns out that these two tests are identical; and so I’ve collapsed them together.<a href="chisquare.html#fnref177">↩</a></p></li>
<li id="fn178"><p>Technically, <span class="math inline">\(E_{ij}\)</span> here is an estimate, so I should probably write it <span class="math inline">\(\hat{E}_{ij}\)</span>. But since no-one else does, I won’t either.<a href="chisquare.html#fnref178">↩</a></p></li>
<li id="fn179"><p>A problem many of us worry about in real life.<a href="chisquare.html#fnref179">↩</a></p></li>
<li id="fn180"><p>Though I do feel that it’s worth mentioning the <code>assocstats()</code> function in the <code>vcd</code> package. If you install and load the <code>vcd</code> package, then a command like <code>assocstats( chapekFrequencies )</code> will run the <span class="math inline">\(\chi^2\)</span> test as well as the likelihood ratio test (not discussed here); and then report three different measures of effect size: <span class="math inline">\(\phi^2\)</span>, Cram'er’s <span class="math inline">\(V\)</span>, and the contingency coefficient (not discussed here)<a href="chisquare.html#fnref180">↩</a></p></li>
<li id="fn181"><p>Not really.<a href="chisquare.html#fnref181">↩</a></p></li>
<li id="fn182"><p>This example is based on a joke article published in the <em>Journal of Irreproducible Results</em>.<a href="chisquare.html#fnref182">↩</a></p></li>
<li id="fn183"><p>The R functions for this distribution are <code>dhyper()</code>, <code>phyper()</code>, <code>qhyper()</code> and <code>rhyper()</code>, though you don’t need them for this book, and I haven’t given you enough information to use these to perform the Fisher exact test the long way.<a href="chisquare.html#fnref183">↩</a></p></li>
<li id="fn184"><p>Not surprisingly, the Fisher exact test is motivated by Fisher’s interpretation of a <span class="math inline">\(p\)</span>-value, not Neyman’s!<a href="chisquare.html#fnref184">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="part-v-statistical-tools.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ttest.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
